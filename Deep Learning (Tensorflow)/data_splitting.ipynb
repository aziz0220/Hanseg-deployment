{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea0d1d72b4e0c020",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "id": "c51ef570d0e1ecb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:48.600319Z",
     "start_time": "2024-05-05T04:08:48.206289Z"
    }
   },
   "source": [
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/aziz0220/anaconda3/envs/py310/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n",
      "Sun May  5 05:08:48 2024       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 550.76.01              Driver Version: 552.22         CUDA Version: 12.4     |\r\n",
      "|-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 4060 ...    On  |   00000000:01:00.0  On |                  N/A |\r\n",
      "| N/A   59C    P0             20W /  124W |    7810MiB /   8188MiB |      0%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    0   N/A  N/A     22973      C   /python3.10                                 N/A      |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:48.822076Z",
     "start_time": "2024-05-05T04:08:48.819100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Load the TensorBoard notebook extension.\n",
    "# %load_ext tensorboard\n",
    "# from datetime import datetime\n",
    "# from packaging import version\n",
    "# \n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# print(\"TensorFlow version: \", tf.__version__)\n",
    "# assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "#     \"This notebook requires TensorFlow 2.0 or above.\"\n",
    "# \n",
    "# \n",
    "# import tensorboard\n",
    "# tensorboard.__version__"
   ],
   "id": "b386949daac7b743",
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "id": "4c6783cafbeaf24e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:48.881230Z",
     "start_time": "2024-05-05T04:08:48.872569Z"
    }
   },
   "source": [
    "import seg_metrics.seg_metrics as sg\n",
    "from skimage import io\n",
    "from skimage.metrics import hausdorff_distance\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger\n",
    "from matplotlib import pyplot as plt\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "#from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from PIL import Image\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "id": "9776655bd61eacd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:48.971591Z",
     "start_time": "2024-05-05T04:08:48.960533Z"
    }
   },
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=20000)]) \n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            configs  = tf.config.get_logical_device_configuration( gpu )  \n",
    "            print(gpu, \" :::::::::::: \", logical_gpus , \" :::::: \", configs)\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')  ::::::::::::  [LogicalDevice(name='/device:GPU:0', device_type='GPU')]  ::::::  [LogicalDeviceConfiguration(memory_limit=20000, experimental_priority=None, experimental_device_ordinal=None)]\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "id": "ce1c427311ede2c4",
   "metadata": {},
   "source": [
    "# SETTING GLOBAL VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f01340f44689bcf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.031823Z",
     "start_time": "2024-05-05T04:08:49.015786Z"
    }
   },
   "source": [
    "global IMG_H\n",
    "global IMG_W\n",
    "global IMG_D\n",
    "global NUM_CLASSES\n",
    "global CLASSES\n",
    "global image_h\n",
    "global image_w\n",
    "global num_classes\n",
    "global classes\n",
    "global rgb_codes\n",
    "global COLORMAP\n",
    "global case_names\n",
    "global dataset_path\n",
    "global input_shape\n",
    "global dataOutputPath\n",
    "image_h = 512\n",
    "image_w = 512\n",
    "num_classes = 31\n",
    "input_shape = (image_h, image_w, 3)\n",
    "batch_size = 2\n",
    "lr = 1e-4\n",
    "num_epochs = 7\n",
    "HOUNSFIELD_MIN = -1000\n",
    "HOUNSFIELD_MAX = 2000\n",
    "HOUNSFIELD_RANGE = HOUNSFIELD_MAX - HOUNSFIELD_MIN\n",
    "SLICE_DECIMATE_IDENTIFIER = 3\n",
    "SLICE_X = True\n",
    "SLICE_Y = False\n",
    "SLICE_Z = False\n",
    "dataset_path = '/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/'\n",
    "os.chdir(dataset_path)\n",
    "input_path =dataset_path + 'set_2/'\n",
    "case_names = [f\"case_{num:02d}\" for num in range(1, 43)]\n",
    "dataOutputPath =dataset_path + 'set_4/'\n",
    "imageSliceOutput = os.path.join(dataOutputPath, 'img/')\n",
    "maskSliceOutput = os.path.join(dataOutputPath, 'mask/')\n",
    "trainsetOutput = os.path.join(dataOutputPath, 'train/')\n",
    "testsetOutput = os.path.join(dataOutputPath, 'test/')\n",
    "dataset_path = '/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4'\n",
    "model_path = os.path.join(\"files\", \"model.h5\")\n",
    "csv_path = os.path.join(\"files\", \"data.csv\")\n",
    "SEED = 909\n",
    "BATCH_SIZE_TRAIN = 2\n",
    "BATCH_SIZE_TEST = 2\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 512\n",
    "IMG_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "data_dir = '/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4/'\n",
    "data_dir_train = os.path.join(data_dir, 'train/')\n",
    "data_dir_train_image = os.path.join(data_dir_train, 'images/')\n",
    "data_dir_train_mask = os.path.join(data_dir_train, 'masks/')\n",
    "data_dir_test = os.path.join(data_dir, 'test/')\n",
    "data_dir_test_image = os.path.join(data_dir_test, 'images/')\n",
    "data_dir_test_mask = os.path.join(data_dir_test, 'masks/')\n",
    "NUM_OF_EPOCHS = 10\n",
    "LABEL_dict = {\n",
    "    \"background\": 0,\n",
    "    \"A_Carotid_L\": 1,\n",
    "    \"A_Carotid_R\": 2,\n",
    "    \"Arytenoid\": 3,\n",
    "    \"Bone_Mandible\": 4,\n",
    "    \"Brainstem\": 5,\n",
    "    \"BuccalMucosa\": 6,\n",
    "    \"Cavity_Oral\": 7,\n",
    "    \"Cochlea_L\": 8,\n",
    "    \"Cochlea_R\": 9,\n",
    "    \"Cricopharyngeus\": 10,\n",
    "    \"Esophagus_S\": 11,\n",
    "    \"Eye_AL\": 12,\n",
    "    \"Eye_AR\": 13,\n",
    "    \"Eye_PL\": 14,\n",
    "    \"Eye_PR\": 15,\n",
    "    \"Glnd_Lacrimal_L\": 16,\n",
    "    \"Glnd_Lacrimal_R\": 17,\n",
    "    \"Glnd_Submand_L\": 18,\n",
    "    \"Glnd_Submand_R\": 19,\n",
    "    \"Glnd_Thyroid\": 20,\n",
    "    \"Glottis\": 21,\n",
    "    \"Larynx_SG\": 22,\n",
    "    \"Lips\": 23,\n",
    "    \"OpticChiasm\": 24,\n",
    "    \"OpticNrv_L\": 25,\n",
    "    \"OpticNrv_R\": 26,\n",
    "    \"Parotid_L\": 27,\n",
    "    \"Parotid_R\": 28,\n",
    "    \"Pituitary\": 29,\n",
    "    \"SpinalCord\": 30,\n",
    "}\n",
    "rgb_codes = [\n",
    "    [255, 255, 255],          # Background\n",
    "            [244, 214, 49],    # SpinalCord\n",
    "            [216, 101, 79],    # A_Carotid_L\n",
    "            [216, 101, 79],    # A_Carotid_R\n",
    "            [183, 156, 220],   # Arytenoid\n",
    "            [222, 198, 101],   # Bone_Mandible\n",
    "            [145, 92, 109],    # Brainstem\n",
    "            [178, 69, 182],    # BuccalMucosa\n",
    "            [121, 39, 153],    # Cavity_Oral\n",
    "            [104, 181, 63],    # Cochlea_L\n",
    "            [123, 174, 91],    # Cochlea_R\n",
    "            [220, 127, 211],   # Cricopharyngeus\n",
    "            [174, 125, 64],    # Esophagus_S\n",
    "            [127, 75, 38],     # Eye_AL\n",
    "            [127, 75, 38],     # Eye_AR\n",
    "            [53, 152, 174],    # Eye_PL\n",
    "            [53, 152, 174],    # Eye_PR\n",
    "            [86, 58, 127],     # Glnd_Lacrimal_L\n",
    "            [86, 58, 127],     # Glnd_Lacrimal_R\n",
    "            [222, 198, 101],   # Glnd_Submand_L\n",
    "            [222, 198, 101],   # Glnd_Submand_R\n",
    "            [62, 162, 114],    # Glnd_Thyroid\n",
    "            [47, 210, 120],    # Glottis\n",
    "            [150, 208, 243],   # Larynx_SG\n",
    "            [188, 91, 95],     # Lips\n",
    "            [99, 106, 24],     # OpticChiasm\n",
    "            [127, 24, 70],     # OpticNrv_L\n",
    "            [127, 24, 70],     # OpticNrv_R\n",
    "            [31, 45, 172],     # Parotid_L\n",
    "            [31, 45, 172],     # Parotid_R\n",
    "            [57, 157, 110]  \n",
    "]\n",
    "classes = [\n",
    "    \"background\",  \n",
    "            \"A_Carotid_L\",\n",
    "            \"A_Carotid_R\",\n",
    "            \"Arytenoid\",\n",
    "            \"Bone_Mandible\",\n",
    "            \"Brainstem\",\n",
    "            \"BuccalMucosa\",\n",
    "            \"Cavity_Oral\",\n",
    "            \"Cochlea_L\",\n",
    "            \"Cochlea_R\",\n",
    "            \"Cricopharyngeus\",\n",
    "            \"Esophagus_S\",\n",
    "            \"Eye_AL\",\n",
    "            \"Eye_AR\",\n",
    "            \"Eye_PL\",\n",
    "            \"Eye_PR\",\n",
    "            \"Glnd_Lacrimal_L\",\n",
    "            \"Glnd_Lacrimal_R\",\n",
    "            \"Glnd_Submand_L\",\n",
    "            \"Glnd_Submand_R\",\n",
    "            \"Glnd_Thyroid\",\n",
    "            \"Glottis\",\n",
    "            \"Larynx_SG\",\n",
    "            \"Lips\",\n",
    "            \"OpticChiasm\",\n",
    "            \"OpticNrv_L\",\n",
    "            \"OpticNrv_R\",\n",
    "            \"Parotid_L\",\n",
    "            \"Parotid_R\",\n",
    "            \"Pituitary\",\n",
    "            \"SpinalCord\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "id": "48d608b3349870b0",
   "metadata": {},
   "source": [
    "# FUNCTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f282f93960e80",
   "metadata": {},
   "source": [
    " 1. LOADING DATA FUNCTION (from one case)"
   ]
  },
  {
   "cell_type": "code",
   "id": "3508dbc965cc4475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.109956Z",
     "start_time": "2024-05-05T04:08:49.103648Z"
    }
   },
   "source": [
    "def load_data(case_number):\n",
    "    CT_volume = io.imread(input_path + f\"/{case_names[case_number]}/{case_names[case_number]}_IMG_CT.nrrd\")\n",
    "    MR_volume = io.imread(input_path + f\"/{case_names[case_number]}/{case_names[case_number]}_IMG_MR_T1.nrrd\")\n",
    "    mask = io.imread(input_path + f\"/{case_names[case_number]}/{case_names[case_number]}_stacked_segments.seg.nrrd\")\n",
    "    print(f\"Case number {case_number} is being processed : Mask shape = {mask.shape}  || CT shape = {CT_volume.shape} || MR shape = {MR_volume.shape}\")\n",
    "    return CT_volume, MR_volume, mask"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "id": "1744eb7b00b24564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.143574Z",
     "start_time": "2024-05-05T04:08:49.139823Z"
    }
   },
   "source": [
    "def load_data_sitk(case_number):\n",
    "    CT_volume = sitk.ReadImage(input_path + f\"/{case_names[case_number]}/{case_names[case_number]}_IMG_CT.nrrd\")\n",
    "    MR_volume = sitk.ReadImage(input_path + f\"/{case_names[case_number]}/{case_names[case_number]}_IMG_MR_T1.nrrd\")\n",
    "    mask = sitk.ReadImage(input_path + f\"/{case_names[case_number]}/{case_names[case_number]}_stacked_segments.seg.nrrd\")\n",
    "    return CT_volume, MR_volume, mask"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "markdown",
   "id": "1706a0cf07fb16e0",
   "metadata": {},
   "source": [
    " 2. NORMALIZING INTENSITY FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "2611041f24418327",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.239335Z",
     "start_time": "2024-05-05T04:08:49.235184Z"
    }
   },
   "source": [
    "def normalizeImageIntensityRange(img, clip_min=0, shiftThreshold=1):\n",
    "    minimg=min(img[img > 0].flatten())\n",
    "    #print(f\"Min value is {minimg}, Max value is {max(img.flatten())}, dtype is {img.dtype}\")\n",
    "    shifted_img = img.copy()  # Create a copy to avoid in-place modification\n",
    "    if(minimg > shiftThreshold):  \n",
    "        shifted_img = shifted_img.astype(np.int16)\n",
    "        shifted_img -= minimg\n",
    "        #print(f\"Min value is {min(shifted_img.flatten())}, Max value is {max(shifted_img.flatten())}\")\n",
    "        shifted_img = np.clip(shifted_img, clip_min, None)\n",
    "        #print(f\"Min value is {min(shifted_img.flatten())},Max value is {max(shifted_img.flatten())}, MR SHAPE : {shifted_img.shape} \")\n",
    "    return shifted_img"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "id": "8d815e8e12075e79",
   "metadata": {},
   "source": [
    "3. PLOTTING INDIVIDIAL CLASSES FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "954b06e5f9117ab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.272385Z",
     "start_time": "2024-05-05T04:08:49.266842Z"
    }
   },
   "source": [
    "def plot_individual_classes(mask, LABEL_dict):\n",
    "  # Get unique pixel values and counts (excluding background)\n",
    "  unique_values, counts = np.unique(mask, return_counts=True)\n",
    "\n",
    "  # Iterate through unique values (excluding background)\n",
    "  plt.imshow(mask, cmap='gray')\n",
    "  plt.title(\"Full Mask\")\n",
    "  plt.axis('off')\n",
    "  plt.show()  # Display the plot\n",
    "  plt.close()  # Close the plot window after display\n",
    "\n",
    "  for i, class_value in enumerate(unique_values):\n",
    "    # Create a binary mask for the current class\n",
    "    class_mask = np.zeros_like(mask)\n",
    "    class_mask[mask == class_value] = 255  # Set the pixels of the current class to white\n",
    "    print(class_value)\n",
    "    # Get class name from LABEL_dict (handling potential missing keys)\n",
    "    class_name = [key for key, value in LABEL_dict.items() if value == class_value][0]\n",
    "    # Plot the class mask\n",
    "    plt.imshow(class_mask, cmap='gray')\n",
    "    plt.title(f\"{class_name} (Count: {counts[i]})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()  # Display the plot\n",
    "    plt.close()  # Close the plot window after display\n"
   ],
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "markdown",
   "id": "be95d765b53926f4",
   "metadata": {},
   "source": [
    "CENTERING MR AND CT\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "906b8767cfcdd87c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.356236Z",
     "start_time": "2024-05-05T04:08:49.352523Z"
    }
   },
   "source": [
    "def center_crop_volumes(case_number):\n",
    "    ct_image, mri_image, mask=load_data_sitk(case_number)\n",
    "    #mask = sitk.Cast(sitk.RescaleIntensity(mask), sitk.sitkUInt8)\n",
    "    mr_resampled = sitk.Resample(mri_image, ct_image)\n",
    "    ct_array = sitk.GetArrayFromImage(ct_image)\n",
    "    mri_array = sitk.GetArrayFromImage(mr_resampled)\n",
    "    mask_array = sitk.GetArrayFromImage(mask)\n",
    "    #scaled_mask = (mask_array / 255 * 31)  # Perform scaling\n",
    "    #mask_uint8 = scaled_mask.astype(np.uint8)\n",
    "    return ct_array, mri_array, mask_array"
   ],
   "outputs": [],
   "execution_count": 85
  },
  {
   "cell_type": "markdown",
   "id": "a378442e2805f26e",
   "metadata": {},
   "source": [
    "4. SAVING SLICE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0840055edb88097",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.382062Z",
     "start_time": "2024-05-05T04:08:49.377200Z"
    }
   },
   "source": [
    "def saveSlice(img, fname, path, mode='image'):\n",
    "  os.makedirs(path, exist_ok=True)  # Ensure path exists\n",
    "  #dtyper = str(img.dtype)  # Store original dtype\n",
    "  img = np.flipud(img)\n",
    "  image = Image.fromarray(img) \n",
    "  new_size = max(img.shape[0], img.shape[1])\n",
    "  image = image.resize((new_size, new_size), Image.NEAREST)\n",
    "  width, height = 512, 512  # Desired crop dimensions\n",
    "  left = (image.width - width) // 2\n",
    "  top = (image.height - height) // 2\n",
    "  right = left + width\n",
    "  bottom = top + height\n",
    "  image = image.crop((left, top, right, bottom))\n",
    "  img = np.array(image)\n",
    "  fgs=Image.fromarray(img)\n",
    "  fout = os.path.join(path, f'{fname}.png')\n",
    "  if(mode == 'mask'):\n",
    "    fgs.save(fout)\n",
    "  else:\n",
    "    plt.imsave(fout,fgs,cmap='gray' )\n",
    "  if( is_empty_image(img) and mode == 'image'):\n",
    "    print(f\"Empty image {fout}\")\n",
    "  return img"
   ],
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "id": "284ead73ea05853e",
   "metadata": {},
   "source": [
    "5. SLICING AND SAVING VOLUME FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "4abd6fb540ce6c07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.476018Z",
     "start_time": "2024-05-05T04:08:49.468321Z"
    }
   },
   "source": [
    "def sliceAndSaveVolumeImage(vol, mask, fname, pathimg, pathmask, class_counts, label_dict=LABEL_dict, mode='image'):\n",
    "    (dimx, dimy, dimz) = vol.shape\n",
    "    cnt = 0\n",
    "    empty =0\n",
    "    if SLICE_X:\n",
    "        cnt += dimx\n",
    "        #print('Slicing X: ')\n",
    "        for i in range(dimx):\n",
    "            if(not is_empty_image(vol[i,:,:])):\n",
    "                img=saveSlice(vol[i,:,:], fname+f'-slice{str(i).zfill(SLICE_DECIMATE_IDENTIFIER)}_x', pathimg, mode='image')\n",
    "                msk=saveSlice(mask[i,:,:], fname+f'-slice{str(i).zfill(SLICE_DECIMATE_IDENTIFIER)}_x', pathmask, mode='mask')\n",
    "                class_counts = class_to_dict(msk, class_counts, label_dict) \n",
    "            else:\n",
    "                empty+=1\n",
    "    if SLICE_Y:\n",
    "        cnt += dimy\n",
    "        #print('Slicing Y: ')\n",
    "        for i in range(dimy):\n",
    "            if(not is_empty_image(vol[:,i,:])):\n",
    "                img=saveSlice(vol[:,i,:], fname+f'-slice{str(i).zfill(SLICE_DECIMATE_IDENTIFIER)}_y', pathimg, mode='image')\n",
    "                msk=saveSlice(mask[:,i,:], fname+f'-slice{str(i).zfill(SLICE_DECIMATE_IDENTIFIER)}_y', pathmask, mode='mask')\n",
    "                class_counts = class_to_dict(msk, class_counts, label_dict)     \n",
    "            else:\n",
    "                empty+=1            \n",
    "    if SLICE_Z:\n",
    "        cnt += dimz\n",
    "        #print('Slicing Z: ')\n",
    "        for i in range(dimz):\n",
    "            if(not is_empty_image(vol[:,:,i])):\n",
    "                img=saveSlice(vol[:,:,i], fname+f'-slice{str(i).zfill(SLICE_DECIMATE_IDENTIFIER)}_z', pathimg, mode='image')\n",
    "                msk=saveSlice(mask[:,:,i], fname+f'-slice{str(i).zfill(SLICE_DECIMATE_IDENTIFIER)}_z', pathmask, mode='mask')\n",
    "                class_counts = class_to_dict(msk, class_counts, label_dict)                       \n",
    "            else:\n",
    "                empty+=1\n",
    "    print(f\"Removed {empty} Empty slices out of {cnt} total slices\")\n",
    "    cnt = cnt - empty\n",
    "    return cnt"
   ],
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "markdown",
   "id": "7e76e18e818360d2",
   "metadata": {},
   "source": [
    "6. IS IMAGE EMPTY THRESHOLD FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "ce6a84b7ebad4b25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.508474Z",
     "start_time": "2024-05-05T04:08:49.504180Z"
    }
   },
   "source": [
    "def is_empty_image_combined(image_path, threshold=10):\n",
    "  with Image.open(image_path) as img:\n",
    "    # Convert to grayscale\n",
    "    grayscale_img = img.convert('L')\n",
    "    grayscale_img = np.array(grayscale_img)\n",
    "    uniform = np.all(np.unique(grayscale_img) == grayscale_img[0, 0])\n",
    "    # Check non-zero pixel sum (avoids black image misclassification)\n",
    "    non_zero_sum = np.sum(grayscale_img != 0)\n",
    "    print(f\"Uniform: {uniform}, Non-zero sum: {non_zero_sum}\")\n",
    "    return uniform and (non_zero_sum < threshold)"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "id": "486b4a5b283d1c72",
   "metadata": {},
   "source": [
    "IS EMPTY IMAGE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "857bb88da9bc2441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.583188Z",
     "start_time": "2024-05-05T04:08:49.580553Z"
    }
   },
   "source": [
    "def is_empty_image(img):\n",
    "    return np.all(img == 0) "
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "markdown",
   "id": "82322343251a632c",
   "metadata": {},
   "source": [
    "7. CROP DATA FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "65889837f55ad0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.634960Z",
     "start_time": "2024-05-05T04:08:49.629210Z"
    }
   },
   "source": [
    "def crop_data(CT_volume, MR_volume, mask, crop_type='center'):\n",
    "\n",
    "  # Get minimum overlap dimensions\n",
    "  min_shape = (min(CT_volume.shape[0], MR_volume.shape[0]),\n",
    "               min(CT_volume.shape[1], MR_volume.shape[1]),\n",
    "               min(CT_volume.shape[2], MR_volume.shape[2]))\n",
    "\n",
    "  # Handle different cropping types\n",
    "  if crop_type == 'center':\n",
    "    # Calculate starting indices for centered cropping\n",
    "    start_CT_x = int((CT_volume.shape[1] - min_shape[1]) / 2)\n",
    "    start_CT_y = int((CT_volume.shape[0] - min_shape[0]) / 2)\n",
    "    start_CT_z = int((CT_volume.shape[2] - min_shape[2]) / 2)\n",
    "\n",
    "    start_MR_x = int((MR_volume.shape[1] - min_shape[1]) / 2)\n",
    "    start_MR_y = int((MR_volume.shape[0] - min_shape[0]) / 2)\n",
    "    start_MR_z = int((MR_volume.shape[2] - min_shape[2]) / 2)\n",
    "    \n",
    "    start_MK_x = int((mask.shape[1] - min_shape[1]) / 2)\n",
    "    start_MK_y = int((mask.shape[0] - min_shape[0]) / 2)\n",
    "    start_MK_z = int((mask.shape[2] - min_shape[2]) / 2)\n",
    "\n",
    "    # Crop volumes from center\n",
    "    cropped_CT = CT_volume[start_CT_y:start_CT_y + min_shape[0],\n",
    "                             start_CT_x:start_CT_x + min_shape[1],\n",
    "                             start_CT_z:start_CT_z + min_shape[2]]\n",
    "    cropped_MR = MR_volume[start_MR_y:start_MR_y + min_shape[0],\n",
    "                             start_MR_x:start_MR_x + min_shape[1],\n",
    "                             start_MR_z:start_MR_z + min_shape[2]]\n",
    "    cropped_mask = mask[start_MK_y:start_MK_y + min_shape[0],\n",
    "                                start_MK_x:start_MK_x + min_shape[1],\n",
    "                                start_MK_z:start_MK_z + min_shape[2]]   \n",
    "    #print(f\"CT shape: {cropped_CT.shape}, MR shape: {cropped_MR.shape}, Mask shape: {cropped_mask.shape}\")\n",
    "  elif crop_type == 'min':\n",
    "    # Crop minimum overlap region (might leave black borders)\n",
    "    # Implement logic for minimum overlap cropping here (consider using np.where)\n",
    "    pass\n",
    "  elif crop_type == 'max':\n",
    "    # Crop maximum overlap region (might cut off some data)\n",
    "    # Implement logic for maximum overlap cropping here (consider slicing)\n",
    "    pass\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid crop_type: {crop_type}. Supported options are 'center', 'min', and 'max'.\")\n",
    "    \n",
    "  return cropped_CT, cropped_MR, cropped_mask\n"
   ],
   "outputs": [],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "id": "81acf4d5572ded8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.715187Z",
     "start_time": "2024-05-05T04:08:49.709777Z"
    }
   },
   "source": [
    "def crop_data_v2(CT_volume, MR_volume, mask, original_CT, original_MR, crop_type='center'):\n",
    "\n",
    "  # Get minimum overlap dimensions\n",
    "  min_shape = (min(original_CT[0], original_MR[0]),\n",
    "               min(original_CT[1], original_MR[1]),\n",
    "               min(original_CT[2], original_MR[2]))\n",
    "\n",
    "  # Handle different cropping types\n",
    "  if crop_type == 'center':\n",
    "    unique_classes, counts = np.unique(mask, return_counts=True)\n",
    "    print(len(unique_classes))\n",
    "    # Calculate starting indices for centered cropping\n",
    "    start_CT_x = int((CT_volume.shape[1] - min_shape[1]) / 2)\n",
    "    start_CT_y = int((CT_volume.shape[0] - min_shape[0]) / 2)\n",
    "    start_CT_z = int((CT_volume.shape[2] - min_shape[2]) / 2)\n",
    "\n",
    "    start_MR_x = int((MR_volume.shape[1] - min_shape[1]) / 2)\n",
    "    start_MR_y = int((MR_volume.shape[0] - min_shape[0]) / 2)\n",
    "    start_MR_z = int((MR_volume.shape[2] - min_shape[2]) / 2)\n",
    "    \n",
    "    start_MK_x = int((mask.shape[1] - min_shape[1]) / 2)\n",
    "    start_MK_y = int((mask.shape[0] - min_shape[0]) / 2)\n",
    "    start_MK_z = int((mask.shape[2] - min_shape[2]) / 2)\n",
    "\n",
    "    # Crop volumes from center\n",
    "    cropped_CT = CT_volume[start_CT_y:start_CT_y + min_shape[0],\n",
    "                             start_CT_x:start_CT_x + min_shape[1],\n",
    "                             start_CT_z:start_CT_z + min_shape[2]]\n",
    "    cropped_MR = MR_volume[start_MR_y:start_MR_y + min_shape[0],\n",
    "                             start_MR_x:start_MR_x + min_shape[1],\n",
    "                             start_MR_z:start_MR_z + min_shape[2]]\n",
    "    cropped_mask = mask[start_MK_y:start_MK_y + min_shape[0],\n",
    "                                start_MK_x:start_MK_x + min_shape[1],\n",
    "                                start_MK_z:start_MK_z + min_shape[2]]   \n",
    "    unique_classes, counts = np.unique(cropped_mask, return_counts=True)\n",
    "    print(len(unique_classes))\n",
    "    #print(f\"CT shape: {cropped_CT.shape}, MR shape: {cropped_MR.shape}, Mask shape: {cropped_mask.shape}\")\n",
    "  elif crop_type == 'min':\n",
    "    # Crop minimum overlap region (might leave black borders)\n",
    "    # Implement logic for minimum overlap cropping here (consider using np.where)\n",
    "    pass\n",
    "  elif crop_type == 'max':\n",
    "    # Crop maximum overlap region (might cut off some data)\n",
    "    # Implement logic for maximum overlap cropping here (consider slicing)\n",
    "    pass\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid crop_type: {crop_type}. Supported options are 'center', 'min', and 'max'.\")\n",
    "    \n",
    "  return cropped_CT, cropped_MR, cropped_mask\n"
   ],
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "markdown",
   "id": "56fa28f5b1e3d022",
   "metadata": {},
   "source": [
    "8. CONCATENATE DATA FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "84bb67c201ef3d40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.741829Z",
     "start_time": "2024-05-05T04:08:49.738093Z"
    }
   },
   "source": [
    "# def concatenate_data(concatenated_volume ,cropped_ct_image, MR_volume, Masks):\n",
    "#     if(np.all(Masks == 0) and np.all(concatenated_volume == 0)):\n",
    "#         concatenated_volume = np.concatenate((cropped_ct_image, MR_volume), axis=0)\n",
    "#         Masks = np.concatenate((cropped_mask, cropped_mask), axis=0)\n",
    "#         #print(f\"Volume shape is {concatenated_volume.shape}  \")\n",
    "#         return concatenated_volume, Masks\n",
    "#     concatenated_volume = np.concatenate((concatenated_volume, cropped_ct_image), axis=0)\n",
    "#     concatenated_volume = np.concatenate((concatenated_volume, MR_volume), axis=0)\n",
    "#     Masks = np.concatenate((Masks, cropped_mask), axis=0)\n",
    "#     Masks = np.concatenate((Masks, cropped_mask), axis=0)\n",
    "#     #print(f\"Volume shape is {concatenated_volume.shape} \")\n",
    "#     if(concatenated_volume.shape != Masks.shape):\n",
    "#         print(\"Concatenation error\")\n",
    "#     return concatenated_volume, Masks"
   ],
   "outputs": [],
   "execution_count": 92
  },
  {
   "cell_type": "markdown",
   "id": "c11adcd9f02e2f47",
   "metadata": {},
   "source": [
    "9. EXTRACTING CLASSES NUMBER FROM MASK FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "75a81ef71222d2cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.837267Z",
     "start_time": "2024-05-05T04:08:49.833796Z"
    }
   },
   "source": [
    "def class_to_dict (msk, class_counts, label_dict = LABEL_dict):\n",
    "    unique_classes, counts = np.unique(msk, return_counts=True)\n",
    "    for class_value, count in zip(unique_classes, counts):\n",
    "        if class_value in label_dict.values():\n",
    "            class_name = [key for key, value in label_dict.items() if value == class_value][0]\n",
    "            class_counts[class_name] += 1\n",
    "    return class_counts"
   ],
   "outputs": [],
   "execution_count": 93
  },
  {
   "cell_type": "markdown",
   "id": "4397400a17df6eb1",
   "metadata": {},
   "source": [
    "10. LOADING ALL DATA INTO SLICES FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "ded71241b2a686b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.861547Z",
     "start_time": "2024-05-05T04:08:49.855121Z"
    }
   },
   "source": [
    "def load_all_data(start_case = 0, end_case = len(case_names), normalize=False, concatenate=False , target_shape = (512, 512, 512), label_dict = LABEL_dict, imageSliceOutput = imageSliceOutput, maskSliceOutput = maskSliceOutput):\n",
    "    concatenated_volume = np.empty((512, 512, 512))\n",
    "    numOfSlicesCT = 0\n",
    "    numOfSlicesMR = 0\n",
    "    if concatenate:\n",
    "        Masks = np.empty((512, 512, 512))\n",
    "    class_counts = {name: 0 for name in label_dict.keys()}  \n",
    "    for case_number in range(start_case, end_case):\n",
    "        CT_volume_old, MR_volume_old, mask_old= load_data(case_number)      \n",
    "        original_CT = CT_volume_old.shape\n",
    "        original_MR = MR_volume_old.shape\n",
    "        CT_volume, MR_volume, mask = center_crop_volumes(case_number)\n",
    "        #cropped_ct_image, cropped_mr_image, cropped_mask =crop_data_v2(CT_volume, MR_volume, mask, original_CT, original_MR)   \n",
    "        cropped_ct_image, cropped_mr_image, cropped_mask = CT_volume, MR_volume, mask\n",
    "        if normalize:\n",
    "            cropped_mr_image=normalizeImageIntensityRange(cropped_mr_image)\n",
    "            #cropped_ct_image=normalizeImageIntensityRange(cropped_ct_image)\n",
    "        if concatenate:\n",
    "            # concatenated_volume, Masks = concatenate_data(concatenated_volume, cropped_ct_image, cropped_mr_image, cropped_mask)\n",
    "            if(concatenated_volume.shape[0] > target_shape[0]):\n",
    "                concatenated_volume = concatenated_volume[:input_shape[0]]\n",
    "                Masks = Masks[:input_shape[0]]\n",
    "            if(start_case == start_case):\n",
    "                numOfSlicesCTMR = sliceAndSaveVolumeImage(concatenated_volume, f\"case_{case_number}_MR_CT\", imageSliceOutput)\n",
    "            else :\n",
    "                numOfSlicesCTMR = sliceAndSaveVolumeImage(concatenated_volume, f\"case_{start_case}_{case_number}_MR_CT\", imageSliceOutput)\n",
    "            print(f'\\n {numOfSlicesCTMR} CT + MR slices created \\n')\n",
    "            numOfSlicesMask = sliceAndSaveVolumeImage(Masks, f\"case_{case_number}_mask\", maskSliceOutput)\n",
    "            print(f'\\n {numOfSlicesMask} mask slices created \\n')\n",
    "            return case_number\n",
    "        else:\n",
    "            numOfSlicesCT += sliceAndSaveVolumeImage(cropped_ct_image,cropped_mask, f\"case_{case_number}_CT\", imageSliceOutput, maskSliceOutput,class_counts = class_counts)\n",
    "            # numOfSlicesMR += sliceAndSaveVolumeImage(cropped_mr_image, cropped_mask, f\"case_{case_number}_MR\", imageSliceOutput, maskSliceOutput, class_counts = class_counts, mode='imageMR')\n",
    "    print(f'\\n {numOfSlicesMR} MR slices created \\n')\n",
    "    print(f'\\n {numOfSlicesCT} CT slices created \\n')\n",
    "    print(f'\\n {numOfSlicesCT+numOfSlicesMR} mask slices created \\n')\n",
    "    print(f\"\\n Class counts: {class_counts} \\n\")\n",
    "    return end_case"
   ],
   "outputs": [],
   "execution_count": 94
  },
  {
   "cell_type": "markdown",
   "id": "b72e54b805d78378",
   "metadata": {},
   "source": [
    "11. COUNTING EMPTY MASKS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "de331fdf6fb5d9ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.888610Z",
     "start_time": "2024-05-05T04:08:49.883721Z"
    }
   },
   "source": [
    "def count_empty_masks(mask_folder):\n",
    "  \"\"\"\n",
    "  Counts the number of empty masks in a given folder.\n",
    "\n",
    "  Args:\n",
    "      mask_folder (str): The path to the folder containing masks.\n",
    "\n",
    "  Returns:\n",
    "      tuple: A tuple containing the number of empty masks and total masks.\n",
    "  \"\"\"\n",
    "\n",
    "  total_masks = 0\n",
    "  empty_masks = 0\n",
    "\n",
    "  for filename in os.listdir(mask_folder):\n",
    "    if filename.endswith('.png'):  # Adjust for your mask format\n",
    "      mask_path = os.path.join(mask_folder, filename)\n",
    "      mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "      #mask_img = Image.open(mask_path)\n",
    "      #mask = np.array(mask_img)\n",
    "      # Check if all pixels are black (empty mask)\n",
    "      if np.all(mask == 0):\n",
    "        empty_masks += 1\n",
    "    total_masks += 1\n",
    "\n",
    "  return empty_masks, total_masks"
   ],
   "outputs": [],
   "execution_count": 95
  },
  {
   "cell_type": "markdown",
   "id": "f10a7a073eaa0ce6",
   "metadata": {},
   "source": [
    "12. GET NON EMPTY IMAGES FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8dad1c1fae1538a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:49.987148Z",
     "start_time": "2024-05-05T04:08:49.983210Z"
    }
   },
   "source": [
    "def get_non_empty_images(image_path):\n",
    "  if not os.path.isdir(image_path):\n",
    "    raise ValueError(\"Provided path is not a directory\")\n",
    "  non_empty_images = []\n",
    "  # Loop through all files in the directory\n",
    "  for filename in os.listdir(image_path):\n",
    "    if filename.lower().endswith((\".png\")):\n",
    "      full_path = os.path.join(image_path, filename)\n",
    "      mask = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE) \n",
    "      #mask_img = Image.open(full_path)\n",
    "      #mask = np.array(mask_img)\n",
    "      # Check if all pixels are black (empty mask)\n",
    "      if not np.all(mask == 0):\n",
    "        non_empty_images.append(filename)\n",
    "  return non_empty_images"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "markdown",
   "id": "91d6f4f40e10383c",
   "metadata": {},
   "source": [
    "13. COUNT MASK CLASSES FROM PATH FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "d003044882d3c638",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.022927Z",
     "start_time": "2024-05-05T04:08:50.018906Z"
    }
   },
   "source": [
    "def count_classes_from_mask(mask_folder, filename):\n",
    "    if filename.endswith('.png'):\n",
    "        mask_path = os.path.join(mask_folder, filename)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        num_classes = count_mask_classes(mask)\n",
    "    return num_classes"
   ],
   "outputs": [],
   "execution_count": 97
  },
  {
   "cell_type": "markdown",
   "id": "1dd863fd597fdfea",
   "metadata": {},
   "source": [
    "14. COUNT MASK CLASSES FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "id": "25778a49df548d68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.115719Z",
     "start_time": "2024-05-05T04:08:50.112642Z"
    }
   },
   "source": [
    "def count_mask_classes(mask):\n",
    "  foreground_mask = mask[mask >= 0]\n",
    "  # Count the number of unique values (including background)\n",
    "  num_classes = len(np.unique(foreground_mask))\n",
    "  print(f\"Number of classes: {num_classes}\")\n",
    "  return num_classes"
   ],
   "outputs": [],
   "execution_count": 98
  },
  {
   "cell_type": "markdown",
   "id": "9694b87c444b43c2",
   "metadata": {},
   "source": [
    "COUNT MASK CLASSES PER CLASS from path"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a0fb60d43aa5f14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.162511Z",
     "start_time": "2024-05-05T04:08:50.158994Z"
    }
   },
   "source": [
    "def count_mask_per_class( path, filenames, label_dict = LABEL_dict): \n",
    "  class_counts = {name: 0 for name in label_dict.keys()}\n",
    "  for filename in filenames:\n",
    "    if filename.endswith('.png'):\n",
    "      mask_path = os.path.join(path, filename)\n",
    "      mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "      #mask = mask.astype(np.float32) / 255.0\n",
    "      class_counts = class_to_dict(mask, class_counts, label_dict)                 \n",
    "  return class_counts"
   ],
   "outputs": [],
   "execution_count": 99
  },
  {
   "cell_type": "markdown",
   "id": "e73b0226c07231a3",
   "metadata": {},
   "source": [
    "COUNT MASKs CLASSES from path"
   ]
  },
  {
   "cell_type": "code",
   "id": "12e07d4d1175b20f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.241792Z",
     "start_time": "2024-05-05T04:08:50.238249Z"
    }
   },
   "source": [
    "def count_masks_per_class(mask_dir, label_dict):\n",
    "  class_counts = {name: 0 for name in label_dict.keys()}\n",
    "  for filename in os.listdir(mask_dir):\n",
    "    if filename.endswith('.png'):\n",
    "      mask_path = os.path.join(mask_dir, filename)\n",
    "      mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "      class_counts = class_to_dict(mask, class_counts, label_dict)         \n",
    "      #class_counts = delete_zero_count_classes(class_counts)\n",
    "  return class_counts"
   ],
   "outputs": [],
   "execution_count": 100
  },
  {
   "cell_type": "code",
   "id": "46642d46fd80cc1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.271740Z",
     "start_time": "2024-05-05T04:08:50.268834Z"
    }
   },
   "source": [
    "def delete_zero_count_classes(class_counts):\n",
    "  return {key: value for key, value in class_counts.items() if value > 0}"
   ],
   "outputs": [],
   "execution_count": 101
  },
  {
   "cell_type": "code",
   "id": "74fbbff8c921e76c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.352715Z",
     "start_time": "2024-05-05T04:08:50.347289Z"
    }
   },
   "source": [
    "def extract_class_masks(mask_folder_path, label_dict):\n",
    "  for filename in os.listdir(mask_folder_path):\n",
    "    if filename.lower().endswith(\".png\"):  # Check for PNG format\n",
    "      mask_path = os.path.join(mask_folder_path, filename)\n",
    "      mask_img = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Read as grayscale\n",
    "      unique_classes, counts = np.unique(mask_img, return_counts=True)\n",
    "      for class_value, count in zip(unique_classes, counts):\n",
    "        if class_value in label_dict.values():\n",
    "            class_name = [key for key, value in label_dict.items() if value == class_value][0]\n",
    "            class_mask = np.zeros_like(mask_img, dtype=np.uint8) \n",
    "            class_mask[mask_img == class_value] = 255\n",
    "            class_folder_path = os.path.join(mask_folder_path, class_name)\n",
    "            if not os.path.exists(class_folder_path):\n",
    "              os.makedirs(class_folder_path)\n",
    "            # Save the extracted class mask with the original filename\n",
    "            class_mask_path = os.path.join(class_folder_path, filename)\n",
    "            cv2.imwrite(class_mask_path, class_mask)"
   ],
   "outputs": [],
   "execution_count": 102
  },
  {
   "cell_type": "code",
   "id": "7a20a73b6ea4c871",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.382315Z",
     "start_time": "2024-05-05T04:08:50.379515Z"
    }
   },
   "source": [
    "def create_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ],
   "outputs": [],
   "execution_count": 103
  },
  {
   "cell_type": "code",
   "id": "885bfe5278b0156c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.462341Z",
     "start_time": "2024-05-05T04:08:50.458063Z"
    }
   },
   "source": [
    "def load_dataset(path):\n",
    "    train_x = sorted(glob(os.path.join(path, \"train\", \"images/images\", \"*.png\")))\n",
    "    train_y = sorted(glob(os.path.join(path, \"train\", \"masks/masks\", \"*.png\")))\n",
    "\n",
    "    valid_x = sorted(glob(os.path.join(path, \"val\", \"images/images\", \"*.png\")))\n",
    "    valid_y = sorted(glob(os.path.join(path, \"val\", \"masks/masks\", \"*.png\")))\n",
    "\n",
    "    test_x = sorted(glob(os.path.join(path, \"test\", \"images/images\", \"*.png\")))\n",
    "    test_y = sorted(glob(os.path.join(path, \"test\", \"masks/masks\", \"*.png\")))\n",
    "\n",
    "    return (train_x, train_y), (valid_x, valid_y), (test_x, test_y)"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "cell_type": "code",
   "id": "ae8c6709a0763cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.516719Z",
     "start_time": "2024-05-05T04:08:50.512966Z"
    }
   },
   "source": [
    "def read_image_mask(x, y):\n",
    "    \"\"\" Image \"\"\"\n",
    "    x = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "    #x = cv2.resize(x, (image_w, image_h))\n",
    "    #x = 1-x \n",
    "    x = x / 255.0\n",
    "    x = x.astype(np.float32)\n",
    "    #x = x.astype(np.uint8)\n",
    "    #x = np.stack((x,)*3, axis=-1)\n",
    "    #print(x.shape, x.dtype)\n",
    "    \"\"\" Mask \"\"\"\n",
    "    y = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "    #y = cv2.resize(y, (image_w, image_h))\n",
    "    y = y.astype(np.int32)\n",
    "    \n",
    "    #x, y = random_flip_left_right(x, y)  # Using TensorFlow's augmentation\n",
    "    #x, y = data_aug(x[np.newaxis, ...], y[np.newaxis, ...])[0]\n",
    "    return x, y"
   ],
   "outputs": [],
   "execution_count": 105
  },
  {
   "cell_type": "code",
   "id": "1a1ff4d2bc2fd943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.588221Z",
     "start_time": "2024-05-05T04:08:50.576543Z"
    }
   },
   "source": [
    "data_aug = ImageDataGenerator(\n",
    "    rotation_range=90,  # Randomly rotate images by up to 20 degrees\n",
    "    width_shift_range=0.2,  # Randomly shift images horizontally by up to 20% of their width\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically by up to 20% of their height\n",
    "    shear_range=0.2,  # Randomly apply shearing transformation\n",
    "    zoom_range=0.2,  # Randomly zoom images in or out by up to 20%\n",
    "    horizontal_flip=True  # Randomly flip images horizontally with a 50% chance\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": 106
  },
  {
   "cell_type": "code",
   "id": "b48745f5a0475013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.622659Z",
     "start_time": "2024-05-05T04:08:50.618815Z"
    }
   },
   "source": [
    "def preprocess(x, y):\n",
    "    def f(x, y):\n",
    "        x = x.decode()\n",
    "        y = y.decode()\n",
    "        return read_image_mask(x, y)\n",
    "    \n",
    "    image, mask = tf.numpy_function(f, [x, y], [tf.float32, tf.int32])\n",
    "    mask = tf.one_hot(mask, num_classes)\n",
    "    image.set_shape([image_h, image_w, 3])\n",
    "    mask.set_shape([image_h, image_w, num_classes])\n",
    "\n",
    "    return image, mask"
   ],
   "outputs": [],
   "execution_count": 107
  },
  {
   "cell_type": "code",
   "id": "b4b9d2378892a407",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.703321Z",
     "start_time": "2024-05-05T04:08:50.695928Z"
    }
   },
   "source": [
    "class Augment(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    self.seed = seed\n",
    "    \n",
    "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "    \n",
    "    self.rotate_inputs = tf.keras.layers.RandomRotation(factor=0.2, seed=self.seed)\n",
    "    self.rotate_labels = tf.keras.layers.RandomRotation(factor=0.2, seed=self.seed)\n",
    "  def call(self, inputs, labels):\n",
    "    # Apply horizontal flip first\n",
    "    inputs = self.augment_inputs(inputs)\n",
    "    labels = self.augment_labels(labels)\n",
    "\n",
    "    # Apply random rotation afterwards (on the flipped images)\n",
    "    inputs = self.rotate_inputs(inputs)\n",
    "    labels = self.rotate_labels(labels)\n",
    "\n",
    "    return inputs, labels\n"
   ],
   "outputs": [],
   "execution_count": 108
  },
  {
   "cell_type": "code",
   "id": "16e09ad0eb2e7951",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.767060Z",
     "start_time": "2024-05-05T04:08:50.763368Z"
    }
   },
   "source": [
    "def tf_dataset_train(X, Y, batch=8):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    ds = ds.shuffle(buffer_size=1000).map(preprocess)\n",
    "    ds = ds.batch(batch).prefetch(2)\n",
    "    ds = ds.map(Augment())\n",
    "    return ds"
   ],
   "outputs": [],
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "id": "71a7888e9d543bd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.850263Z",
     "start_time": "2024-05-05T04:08:50.847021Z"
    }
   },
   "source": [
    "def tf_dataset_test(X, Y, batch=8):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    ds = ds.shuffle(buffer_size=1000).map(preprocess)\n",
    "    ds = ds.batch(batch).prefetch(2)\n",
    "    ds = ds.map(Augment())\n",
    "    return ds"
   ],
   "outputs": [],
   "execution_count": 110
  },
  {
   "cell_type": "code",
   "id": "2410db89d733ce3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.881272Z",
     "start_time": "2024-05-05T04:08:50.877372Z"
    }
   },
   "source": [
    "def grayscale_to_rgb(mask, rgb_codes):\n",
    "    h, w = mask.shape[0], mask.shape[1]\n",
    "    mask = mask.astype(np.int32)\n",
    "    output = []\n",
    "    for i, pixel in enumerate(mask.flatten()):\n",
    "        output.append(rgb_codes[pixel])\n",
    "    output = np.reshape(output, (h, w, 3))\n",
    "    return output"
   ],
   "outputs": [],
   "execution_count": 111
  },
  {
   "cell_type": "code",
   "id": "b6505282a4b5cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:50.943014Z",
     "start_time": "2024-05-05T04:08:50.937492Z"
    }
   },
   "source": [
    "def save_results(image_x, mask, pred, save_image_path):\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    mask = grayscale_to_rgb(mask, rgb_codes)\n",
    "    pred = np.expand_dims(pred, axis=-1)\n",
    "    pred = grayscale_to_rgb(pred, rgb_codes)\n",
    "    if mask.dtype != np.uint8:\n",
    "        mask = mask.astype(np.uint8)\n",
    "    if pred.dtype != np.uint8:\n",
    "        pred = pred.astype(np.uint8)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX  # Choose a suitable font\n",
    "    font_scale = 0.5\n",
    "    font_thickness = 1\n",
    "    text_color = (0, 0, 0)  # Black text color\n",
    "\n",
    "    # Add labels to each image component\n",
    "    image_x_label = cv2.putText(image_x.copy(), \"Image\", (10, 20), font, font_scale, (255, 255, 255) , font_thickness)\n",
    "    mask_label = cv2.putText(mask, \"Mask\", (10, 20), font, font_scale, text_color, font_thickness)\n",
    "    pred_label = cv2.putText(pred, \"Prediction\", (10, 20), font, font_scale, text_color, font_thickness)\n",
    "\n",
    "    line = np.ones((image_x.shape[0], 10, 3)) * 255\n",
    "\n",
    "    cat_images = np.concatenate([image_x_label, line, mask_label, line, pred_label], axis=1)\n",
    "    cv2.imwrite(save_image_path, cat_images)\n"
   ],
   "outputs": [],
   "execution_count": 112
  },
  {
   "cell_type": "code",
   "id": "5bba443e1ffd4f43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.004407Z",
     "start_time": "2024-05-05T04:08:50.998435Z"
    }
   },
   "source": [
    "def conv_block(inputs, num_filters):\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def encoder_block(inputs, num_filters):\n",
    "    x = conv_block(inputs, num_filters)\n",
    "    p = MaxPool2D((2, 2))(x)\n",
    "    return x, p\n",
    "\n",
    "def decoder_block(inputs, skip, num_filters):\n",
    "    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(inputs)\n",
    "    x = Concatenate()([x, skip])\n",
    "    x = conv_block(x, num_filters)\n",
    "    return x\n",
    "\n",
    "def build_unet(input_shape, num_classes):\n",
    "    inputs = Input(input_shape)\n",
    "    s1, p1 = encoder_block(inputs, 64)\n",
    "    s2, p2 = encoder_block(p1, 128)\n",
    "    s3, p3 = encoder_block(p2, 256)\n",
    "    s4, p4 = encoder_block(p3, 512)\n",
    "\n",
    "    b1 = conv_block(p4, 1024)\n",
    "\n",
    "    d1 = decoder_block(b1, s4, 512)\n",
    "    d2 = decoder_block(d1, s3, 256)\n",
    "    d3 = decoder_block(d2, s2, 128)\n",
    "    d4 = decoder_block(d3, s1, 64)\n",
    "\n",
    "    outputs = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(d4)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n"
   ],
   "outputs": [],
   "execution_count": 113
  },
  {
   "cell_type": "code",
   "id": "103c24248ba46274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.075462Z",
     "start_time": "2024-05-05T04:08:51.068729Z"
    }
   },
   "source": [
    "def plot_image_mask(image_path, mask_path):\n",
    "  \"\"\"\n",
    "  Plots an image and its corresponding mask superimposed on each other.\n",
    "\n",
    "  Args:\n",
    "      image_path (str): Path to the image file.\n",
    "      mask_path (str): Path to the mask file.\n",
    "  \"\"\"\n",
    "\n",
    "  # Read the image and mask\n",
    "  image = cv2.imread(image_path)\n",
    "  mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "  # Resize the mask to match the image size if necessary\n",
    "  if mask.shape[:2] != image.shape[:2]:\n",
    "    mask = cv2.resize(mask, dsize=image.shape[:2], interpolation=cv2.INTER_NEAREST)\n",
    "  # Apply transparency to the mask (adjust alpha for desired opacity)\n",
    "  alpha = 0.5  # Set transparency level (0 for transparent, 1 for opaque)\n",
    "  mask_rgb = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)  # Convert to RGB for transparency\n",
    "  mask_rgb[:, :, 2] = mask_rgb[:, :, 2] * alpha  # Set blue channel for transparency\n",
    "\n",
    "  # Superimpose the mask on the image\n",
    "  superimposed_image = cv2.addWeighted(image,0.5, mask_rgb, 1, 0)\n",
    "\n",
    "  # Plot the image and mask\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.subplot(1, 2, 1)\n",
    "  plt.imshow(image)\n",
    "  plt.title(\"Original Image\")\n",
    "  plt.axis('off')\n",
    "\n",
    "  plt.subplot(1, 2, 2)\n",
    "  plt.imshow(superimposed_image)\n",
    "  plt.title(\"Image with Mask (Superimposed)\")\n",
    "  plt.axis('off')\n",
    "\n",
    "  plt.show()"
   ],
   "outputs": [],
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "id": "b3c1c4b0b3bbe2e",
   "metadata": {},
   "source": [
    "#DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "id": "eee48c366d9d3f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.162704Z",
     "start_time": "2024-05-05T04:08:51.156446Z"
    }
   },
   "source": [
    "def create_segmentation_generator_train(img_path, msk_path, BATCH_SIZE):\n",
    "    data_gen_args = dict(\n",
    "                    rescale=1./255,\n",
    "                       featurewise_center=True,\n",
    "                      featurewise_std_normalization=True,\n",
    "                      rotation_range=90,\n",
    "                      width_shift_range=0.2,\n",
    "                       height_shift_range=0.2,\n",
    "                      zoom_range=0.2,\n",
    "                        )\n",
    "    datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    img_generator = datagen.flow_from_directory(img_path, target_size=IMG_SIZE, class_mode=None, color_mode='grayscale', batch_size=BATCH_SIZE, seed=SEED)\n",
    "    msk_generator = datagen.flow_from_directory(msk_path, target_size=IMG_SIZE, class_mode=None, color_mode='grayscale', batch_size=BATCH_SIZE, seed=SEED)\n",
    "    return zip(img_generator, msk_generator)"
   ],
   "outputs": [],
   "execution_count": 115
  },
  {
   "cell_type": "code",
   "id": "3cbac5162026a9a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.221328Z",
     "start_time": "2024-05-05T04:08:51.217599Z"
    }
   },
   "source": [
    "# Remember not to perform any image augmentation in the test generator!\n",
    "def create_segmentation_generator_test(img_path, msk_path, BATCH_SIZE):\n",
    "    data_gen_args = dict(\n",
    "         #              rescale=1./255\n",
    "                        )\n",
    "    datagen = ImageDataGenerator(**data_gen_args)\n",
    "    \n",
    "    img_generator = datagen.flow_from_directory(img_path, target_size=IMG_SIZE, class_mode=None, color_mode='grayscale', batch_size=BATCH_SIZE, seed=SEED)\n",
    "    msk_generator = datagen.flow_from_directory(msk_path, target_size=IMG_SIZE, class_mode=None, color_mode='grayscale', batch_size=BATCH_SIZE, seed=SEED)\n",
    "    return zip(img_generator, msk_generator)"
   ],
   "outputs": [],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "id": "5f7ddbecd5f4e17b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.264659Z",
     "start_time": "2024-05-05T04:08:51.260559Z"
    }
   },
   "source": [
    "def display(display_list):\n",
    "    plt.figure(figsize=(15,15))\n",
    "    \n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "    \n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]), cmap='gray')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "id": "3a3fe7915b2f729",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.340929Z",
     "start_time": "2024-05-05T04:08:51.337103Z"
    }
   },
   "source": [
    "def show_dataset(datagen, num=1):\n",
    "    for i in range(0,num):\n",
    "        image,mask = next(datagen)\n",
    "        display([image[0], mask[0]])"
   ],
   "outputs": [],
   "execution_count": 118
  },
  {
   "cell_type": "code",
   "id": "44abdf8a06abdd97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.351920Z",
     "start_time": "2024-05-05T04:08:51.346871Z"
    }
   },
   "source": [
    "def unet(n_levels, initial_features=32, n_blocks=2, kernel_size=3, pooling_size=2, in_channels=1, out_channels=31):\n",
    "    inputs = keras.layers.Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, in_channels))\n",
    "    x = inputs\n",
    "    \n",
    "    convpars = dict(kernel_size=kernel_size, activation='relu', padding='same')\n",
    "    \n",
    "    #downstream\n",
    "    skips = {}\n",
    "    for level in range(n_levels):\n",
    "        for _ in range(n_blocks):\n",
    "            x = keras.layers.Conv2D(initial_features * 2 ** level, **convpars)(x)\n",
    "        if level < n_levels - 1:\n",
    "            skips[level] = x\n",
    "            x = keras.layers.MaxPool2D(pooling_size)(x)\n",
    "            \n",
    "    # upstream\n",
    "    for level in reversed(range(n_levels-1)):\n",
    "        x = keras.layers.Conv2DTranspose(initial_features * 2 ** level, strides=pooling_size, **convpars)(x)\n",
    "        x = keras.layers.Concatenate()([x, skips[level]])\n",
    "        for _ in range(n_blocks):\n",
    "            x = keras.layers.Conv2D(initial_features * 2 ** level, **convpars)(x)\n",
    "    # output (modified for 31 classes)\n",
    "    activation = 'softmax'  # Use softmax for multi-class segmentation\n",
    "    x = keras.layers.Conv2D(out_channels, kernel_size=1, activation=activation, padding='same')(x)\n",
    "    return keras.Model(inputs=[inputs], outputs=[x], name=f'UNET-L{n_levels}-F{initial_features}')\n"
   ],
   "outputs": [],
   "execution_count": 119
  },
  {
   "cell_type": "code",
   "id": "d6b1f1482d92b367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.404825Z",
     "start_time": "2024-05-05T04:08:51.401714Z"
    }
   },
   "source": [
    "def load_or_build_unet(model_path, input_shape, num_classes):\n",
    "\n",
    "  try:\n",
    "    # Try to load the model from the specified path\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(f\"Loaded existing model from {model_path}\")\n",
    "  except OSError:\n",
    "    # If the file doesn't exist, build a new U-Net model\n",
    "    print(f\"Model not found at {model_path}. Building a new model...\")\n",
    "    print(f\"Input shape: {input_shape}, Number of classes: {num_classes}\")\n",
    "    model = build_unet(input_shape, num_classes)\n",
    "\n",
    "  return model\n"
   ],
   "outputs": [],
   "execution_count": 120
  },
  {
   "cell_type": "code",
   "id": "78e6b3c620b9a153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.495519Z",
     "start_time": "2024-05-05T04:08:51.489023Z"
    }
   },
   "source": [
    "def predict_case(model, nrrd_path, x_start=None, x_end=None, y_start=None, \n",
    "                  y_end=None, z_start=None, z_end=None, output_path=None):\n",
    "\n",
    "\n",
    "  # Read the nrrd data and header\n",
    "  data, header = nrrd.read(nrrd_path)\n",
    "\n",
    "  # Extract data dimensions\n",
    "  data_shape = data.shape\n",
    "\n",
    "  # Define slicing ranges (use full volume if not specified)\n",
    "  x_slice = slice(x_start if x_start is not None else 0, \n",
    "                  x_end if x_end is not None else data_shape[0])\n",
    "  y_slice = slice(y_start if y_start is not None else 0, \n",
    "                  y_end if y_end is not None else data_shape[1])\n",
    "  z_slice = slice(z_start if z_start is not None else 0, \n",
    "                  z_end if z_end is not None else data_shape[2])\n",
    "\n",
    "  # Slice the data (assumes channel as the last dimension)\n",
    "  sliced_data = data[x_slice, y_slice, z_slice, ...]\n",
    "\n",
    "  # Prepare empty list for predictions\n",
    "  predictions = []\n",
    "\n",
    "  # Loop through slices and predict with the model\n",
    "  for image_slice in sliced_data:\n",
    "\n",
    "    #image_slice = np.expand_dims(image_slice, axis=-1)\n",
    "    #image_slice = resize_image(image_slice, model.input_shape[1], model.input_shape[2])  # Add resize function\n",
    "    img = np.flipud(image_slice)\n",
    "    image = Image.fromarray(img) \n",
    "    new_size = max(img.shape[0], img.shape[1])\n",
    "    image = image.resize((new_size, new_size), Image.NEAREST)\n",
    "    width, height = 512, 512  # Desired crop dimensions\n",
    "    left = (image.width - width) // 2\n",
    "    top = (image.height - height) // 2\n",
    "    right = left + width\n",
    "    bottom = top + height\n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    img = np.array(image)\n",
    "    image = img/255.0\n",
    "    image = np.stack((image,)*3, axis=-1)  ## (H, W, 3)\n",
    "    #image = np.expand_dims(image, axis=0) ## [1, H, W, 3]\n",
    "    image_slice = image.astype(np.float32)\n",
    "\n",
    "    # Make prediction on the resized slice\n",
    "    predicted_slice = model.predict(image_slice[np.newaxis, ...], verbose=0)[0]\n",
    "    #pred = model.predict(image, verbose=0)[0]\n",
    "    # Extract the predicted class (assuming model outputs class probabilities)\n",
    "    predicted_slice = np.argmax(predicted_slice[0], axis=-1)\n",
    "    predicted_slice = predicted_slice.astype(np.int32)\n",
    "    predictions.append(predicted_slice)\n",
    "\n",
    "  # Combine predictions back into a single volume\n",
    "  predicted_volume = np.stack(predictions, axis=0)\n",
    "\n",
    "\n",
    "    \n",
    "  # Update header for predicted data (assuming same data type)\n",
    "  predicted_header = header.copy()\n",
    "  predicted_header['data_file'] = None  # Avoid overwriting original data\n",
    "  predicted_header['data_type'] = nrrd.types[predicted_volume.dtype.name]\n",
    "\n",
    "\n",
    "\n",
    "  # Define output path if not provided\n",
    "  if output_path is None:\n",
    "    output_path = nrrd_path.replace('.nrrd', '_pred.nrrd')\n",
    "\n",
    "  # Save the predicted nrrd file\n",
    "  nrrd.write(output_path, predicted_volume, predicted_header)\n",
    "\n",
    "  print(f\"Prediction saved to: {output_path}\")\n"
   ],
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "id": "7c27848a5140016b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.580926Z",
     "start_time": "2024-05-05T04:08:51.577857Z"
    }
   },
   "source": [
    "#data_dir+'files_2/model_2_epoch.h5'"
   ],
   "outputs": [],
   "execution_count": 122
  },
  {
   "cell_type": "code",
   "id": "85b68eba8f82d947",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.617931Z",
     "start_time": "2024-05-05T04:08:51.614646Z"
    }
   },
   "source": [
    "#model=tf.keras.models.load_model(data_dir+'files_2/model_2_epoch.h5')"
   ],
   "outputs": [],
   "execution_count": 123
  },
  {
   "cell_type": "code",
   "id": "e7d6437da5e32fda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.681079Z",
     "start_time": "2024-05-05T04:08:51.677837Z"
    }
   },
   "source": [
    "#CT_path = input_path+'case_01/case_01_IMG_CT.nrrd'"
   ],
   "outputs": [],
   "execution_count": 124
  },
  {
   "cell_type": "code",
   "id": "b622c10a7c29d258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:08:51.720205Z",
     "start_time": "2024-05-05T04:08:51.716105Z"
    }
   },
   "source": [
    "#predict_case(model, CT_path, output_path='data_dir')"
   ],
   "outputs": [],
   "execution_count": 125
  },
  {
   "cell_type": "markdown",
   "id": "e609cfb4a9c1f8cd",
   "metadata": {},
   "source": [
    "# STEP 1 : DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7959221cc1b52a3d",
   "metadata": {},
   "source": [
    " VOLUMES TO SLICES IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "id": "88e9d3f9322da855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T23:14:53.892108Z",
     "start_time": "2024-05-03T23:14:41.335189Z"
    }
   },
   "source": [
    "# looping through all the cases and transforming the data from 3d into 2d slices (512 x 512) from all directions \n",
    "start_case = 0\n",
    "end_case = 42\n",
    "while(start_case != end_case):\n",
    "    start_case=load_all_data(start_case = start_case, end_case = end_case, normalize=False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case number 0 is being processed : Mask shape = (202, 1024, 1024)  || CT shape = (202, 1024, 1024) || MR shape = (83, 512, 512)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_323479/21607885.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# looping through all the cases and transforming the data from 3d into 2d slices (512 x 512) from all directions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mstart_case\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mend_case\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m42\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mwhile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_case\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mend_case\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0mstart_case\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mload_all_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_case\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstart_case\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_case\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mend_case\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnormalize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_323479/1435303042.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(start_case, end_case, normalize, concatenate, target_shape, label_dict, imageSliceOutput, maskSliceOutput)\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mcase_number\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstart_case\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_case\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0mCT_volume_old\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMR_volume_old\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask_old\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mload_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcase_number\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0moriginal_CT\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCT_volume_old\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m         \u001B[0moriginal_MR\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mMR_volume_old\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m         \u001B[0mCT_volume\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMR_volume\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcenter_crop_volumes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcase_number\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m         \u001B[0;31m#cropped_ct_image, cropped_mr_image, cropped_mask =crop_data_v2(CT_volume, MR_volume, mask, original_CT, original_MR)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m         \u001B[0mcropped_ct_image\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcropped_mr_image\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcropped_mask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mCT_volume\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMR_volume\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mnormalize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_323479/898704002.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(case_number)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcenter_crop_volumes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcase_number\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mct_image\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmri_image\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mload_data_sitk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcase_number\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;31m#mask = sitk.Cast(sitk.RescaleIntensity(mask), sitk.sitkUInt8)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mmr_resampled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msitk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mResample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmri_image\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mct_image\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0mct_array\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msitk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGetArrayFromImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mct_image\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_323479/2419721289.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(case_number)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mload_data_sitk\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcase_number\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mCT_volume\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msitk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReadImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34mf\"/{case_names[case_number]}/{case_names[case_number]}_IMG_CT.nrrd\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0mMR_volume\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msitk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReadImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34mf\"/{case_names[case_number]}/{case_names[case_number]}_IMG_MR_T1.nrrd\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mmask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msitk\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReadImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput_path\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34mf\"/{case_names[case_number]}/{case_names[case_number]}_stacked_segments.seg.nrrd\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mCT_volume\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMR_volume\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/SimpleITK/extra.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(fileName, outputPixelType, imageIO)\u001B[0m\n\u001B[1;32m    371\u001B[0m         \u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSetFileNames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfileName\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    372\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    373\u001B[0m     \u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSetImageIO\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimageIO\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    374\u001B[0m     \u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mSetOutputPixelType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moutputPixelType\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 375\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExecute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/SimpleITK/SimpleITK.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   8426\u001B[0m         \u001B[0mtype\u001B[0m \u001B[0mto\u001B[0m \u001B[0mbe\u001B[0m \u001B[0msame\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mIf\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mpixel\u001B[0m \u001B[0mtype\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0mspecified\u001B[0m \u001B[0mthen\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mitk\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mConvertPixelBuffer\u001B[0m \u001B[0mwill\u001B[0m \u001B[0mbe\u001B[0m \u001B[0mused\u001B[0m \u001B[0mto\u001B[0m \u001B[0mconvert\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mpixels\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   8427\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   8428\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   8429\u001B[0m         \"\"\"\n\u001B[0;32m-> 8430\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_SimpleITK\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mImageFileReader_Execute\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "id": "1cd76c0a6587779d",
   "metadata": {},
   "source": [
    "#class_counts = {name: 0 for name in LABEL_dict.keys()}\n",
    "#filename = 'case_0_CT-slice149_x.png'\n",
    "\n",
    "#if filename.endswith('.png'):\n",
    "#  mask_path = os.path.join(dataset_path+\"/mask\", filename)\n",
    "#  mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "#  class_counts = class_to_dict(mask, class_counts, LABEL_dict)         \n",
    "#print(class_counts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dd9db9aeb4ffdbcd",
   "metadata": {},
   "source": [
    "#msk = cv2.imread(maskSliceOutput + 'case_0_CT-slice149_x.png', cv2.IMREAD_GRAYSCALE)\n",
    "#unique_classes, counts = np.unique(msk, return_counts=True)\n",
    "#print(unique_classes)\n",
    "#plot_individual_classes(msk, LABEL_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5dbf4cda9a34b96a",
   "metadata": {},
   "source": [
    "#image_path = imageSliceOutput  + \"case_0_MR-slice434_y.png\"\n",
    "#mask_path = maskSliceOutput+\"case_0_MR-slice434_y.png\"\n",
    "#plot_image_mask(image_path, mask_path)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "24c854d84850c622",
   "metadata": {},
   "source": [
    "EMPTY DATA ANALYSIS / REMOVE EMPTY DATA"
   ]
  },
  {
   "cell_type": "code",
   "id": "c9a772516427df70",
   "metadata": {},
   "source": [
    "# empty_count, total_count = count_empty_masks(imageSliceOutput)\n",
    "# \n",
    "# if total_count > 0:\n",
    "#   empty_percentage = (empty_count / total_count) * 100\n",
    "#   print(f\"There are {empty_count} empty images out of {total_count} total images.\")\n",
    "#   print(f\"This represents an empty images percentage of {empty_percentage:.2f}%.\")\n",
    "# else:\n",
    "#   print(\"No image found in the specified folder.\")\n",
    "# \n",
    "# # Example usage\n",
    "# empty_count, total_count = count_empty_masks(maskSliceOutput)\n",
    "# \n",
    "# if total_count > 0:\n",
    "#   empty_percentage = (empty_count / total_count) * 100\n",
    "#   print(f\"There are {empty_count} empty masks out of {total_count} total masks.\")\n",
    "#   print(f\"This represents an empty mask percentage of {empty_percentage:.2f}%.\")\n",
    "# else:\n",
    "#   print(\"No masks found in the specified folder.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6447cae4bb095017",
   "metadata": {},
   "source": [
    "non_empty_images=get_non_empty_images(imageSliceOutput)\n",
    "non_empty_masks=get_non_empty_images(maskSliceOutput)\n",
    "non_empty_dataset = set(non_empty_images) & set(non_empty_masks)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a79886f2f28e2e14",
   "metadata": {},
   "source": [
    "# STEP 2 : Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "a3593f45b670fa19",
   "metadata": {},
   "source": [
    "image_files = list(non_empty_dataset)\n",
    "mask_files = image_files\n",
    "\n",
    "image_train, image_test, mask_train, mask_test = train_test_split(image_files, mask_files, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Move the corresponding image and mask files to the training and testing directories\n",
    "for file_name in image_train :\n",
    "    shutil.move(os.path.join(imageSliceOutput, file_name), trainsetOutput + 'images/images')\n",
    "for file_name in mask_train:\n",
    "    shutil.move(os.path.join(maskSliceOutput, file_name), trainsetOutput + 'masks/masks')\n",
    "for file_name in image_test:\n",
    "    shutil.move(os.path.join(imageSliceOutput, file_name), testsetOutput + 'images/images')\n",
    "for file_name in mask_test:\n",
    "    shutil.move(os.path.join(maskSliceOutput, file_name), testsetOutput + 'masks/masks')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "584583e8690b812a",
   "metadata": {},
   "source": [
    "ANALISING empty masks in the training set"
   ]
  },
  {
   "cell_type": "code",
   "id": "f64152d664ba3fce",
   "metadata": {},
   "source": [
    "# # Example usage\n",
    "# empty_count, total_count = count_empty_masks( trainsetOutput + '/masks/masks')\n",
    "# \n",
    "# if total_count > 0:\n",
    "#   empty_percentage = (empty_count / total_count) * 100\n",
    "#   print(f\"There are {empty_count} empty masks out of {total_count} total masks.\")\n",
    "#   print(f\"This represents an empty mask percentage of {empty_percentage:.2f}%.\")\n",
    "# else:\n",
    "#   print(\"No masks found in the specified folder.\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1779621897fe5b1",
   "metadata": {},
   "source": [
    " each mask in his folder"
   ]
  },
  {
   "cell_type": "code",
   "id": "ef36477b09733238",
   "metadata": {},
   "source": [
    "#extract_class_masks(trainsetOutput + '/masks/masks', LABEL_dict)\n",
    "#extract_class_masks(testsetOutput + '/masks/masks', LABEL_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ea5f52d924b9c962",
   "metadata": {},
   "source": [
    "ANALYSING CLASS EMBALANCES"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf865c4730de5c18",
   "metadata": {},
   "source": [
    "# train_classes = count_masks_per_class(data_dir_train_mask+ 'masks/', LABEL_dict)\n",
    "# test_classes = count_masks_per_class(data_dir_test_mask + 'masks/', LABEL_dict)\n",
    "# \n",
    "# print(\"Train set contains the following classes :\",train_classes)\n",
    "# print(\"Test set contains the following classes :\", test_classes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8293f2c2a063c8a9",
   "metadata": {},
   "source": [
    "BLANCING CLASSES ALGORITHM"
   ]
  },
  {
   "cell_type": "code",
   "id": "74f46a4d2213fe37",
   "metadata": {},
   "source": [
    "def filename_classes(mask_dir, label_dict):\n",
    "  class_counts = {name: [] for name in label_dict.keys()}\n",
    "  for filename in os.listdir(mask_dir):\n",
    "    if filename.endswith('.png'):\n",
    "      mask_path = os.path.join(mask_dir, filename)\n",
    "      mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "      class_counts = all_masks_classes(mask, filename, class_counts, label_dict)  # Update the actual class_counts dictionary\n",
    "\n",
    "  return class_counts\n",
    "\n",
    "\n",
    "def all_masks_classes(msk, filename,class_counts, label_dict=LABEL_dict):\n",
    "    unique_classes, counts = np.unique(msk, return_counts=True)\n",
    "    for class_value, count in zip(unique_classes, counts):\n",
    "        if class_value in label_dict.values():\n",
    "            class_name = [key for key, value in label_dict.items() if value == class_value][0]\n",
    "            class_counts[class_name].append(filename)  # Append filename to the list\n",
    "    return class_counts\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c60ddddf075bf9e",
   "metadata": {},
   "source": [
    "# train_classes_per_filenames = filename_classes( data_dir_train_mask+ 'masks/', LABEL_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b02bde040a119789",
   "metadata": {},
   "source": [
    "# len(train_classes_per_filenames['background'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4e650f990cddc96",
   "metadata": {},
   "source": [
    "# def balance_classes (data_path,  LABEL_dict )\n",
    "#     classes_filenames = filename_classes( data_path, LABEL_dict)\n",
    "#     #step 1 : remove background from classes_filenames\n",
    "#     #step 2 : detect low persentage classes (imbalanced classed)\n",
    "#     #perform class balencing | make classes balanced\n",
    "#     #step 3 for each file in the imbalanced class if the mask contains only 2 masks (including background) then augment that mask else if it contain other classes (other than the imbalanced class and the background) then check if the other classes present in the mask are \n",
    "# \n",
    "# \n",
    "#     # if the mask "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf49fee8730c2383",
   "metadata": {},
   "source": [
    "# train_classes_per_filenames"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5b9cd75aa0f8863",
   "metadata": {},
   "source": [
    "\n",
    "def balance_classes(data_path, LABEL_dict):\n",
    "    classes_filenames = filename_classes(data_path, LABEL_dict)\n",
    "\n",
    "    # Step 1: Remove background from classes_filenames\n",
    "    #classes_filenames.pop('background')  # Remove background class\n",
    "\n",
    "    # Step 2: Detect low percentage classes (imbalanced classes)\n",
    "    class_counts = np.array([len(filenames) for filenames in classes_filenames.values()])\n",
    "    majority_count = np.max(class_counts)*0.2\n",
    "    majority_count_list = majority_count.tolist()  # Convert to list\n",
    "    print(majority_count_list)\n",
    "    imbalanced_classes = [\n",
    "        class_name\n",
    "        for class_name, filenames in classes_filenames.items()\n",
    "            \n",
    "            if int(len(filenames)) < int(majority_count_list)  # Access the first element (count is a list with a single element)\n",
    "    ]\n",
    "    print(f\"Minority classes: {imbalanced_classes}\")\n",
    "\n",
    "    # Perform class balancing algorithm\n",
    "    balanced_filenames = {}\n",
    "    for class_name, filenames in classes_filenames.items():\n",
    "        if class_name in imbalanced_classes:  # Apply oversampling or undersampling\n",
    "            sampler = RandomOverSampler(sampling_strategy='not majority')  # Oversample minority classes\n",
    "            # Or use undersampling:\n",
    "            # sampler = RandomUnderSampler(sampling_strategy='majority')\n",
    "            filenames_array = np.array(filenames).reshape(-1, 1)  # Reshape for sampling\n",
    "            filenames_resampled, _ = sampler.fit_resample(filenames_array, filenames_array)\n",
    "            balanced_filenames[class_name] = filenames_resampled.flatten().tolist()  # Convert back to list\n",
    "        else:  # Keep majority classes as-is\n",
    "            balanced_filenames[class_name] = filenames\n",
    "\n",
    "    return balanced_filenames\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9451f05019a65807",
   "metadata": {},
   "source": [
    "# balanced_filenames = balance_classes(data_dir_train_mask+ 'masks/', LABEL_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5ca2797dd4cb0052",
   "metadata": {},
   "source": [
    "# for class_name, filenames in train_classes_per_filenames.items():\n",
    "#     print(f\"Class: {class_name} - Count: {len(filenames)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "23bc92f07f2dcaf8",
   "metadata": {},
   "source": [
    " # for class_name, filenames in balanced_filenames.items():\n",
    " #    print(f\"Class: {class_name} - Count: {len(filenames)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4e71ee5973de63b2",
   "metadata": {},
   "source": [
    "DATA GENERATORS"
   ]
  },
  {
   "cell_type": "code",
   "id": "942ca14699c0a673",
   "metadata": {},
   "source": [
    "# from collections import Counter\n",
    "# \n",
    "# class_weights = {}\n",
    "# masks = os.listdir(data_dir_train_mask + 'masks/')\n",
    "# # Assuming data_dir_train_mask is a list containing paths to your training masks\n",
    "# total_count = len(masks)  # Total number of masks\n",
    "# \n",
    "# for mask_path in masks:\n",
    "#     # Load the mask from the path (replace 'load_mask' with your loading function)\n",
    "#     mask = cv2.imread(data_dir_train_mask + 'masks/'+mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "#     # Count the number of pixels for each class in the mask\n",
    "#     class_counts = Counter(mask.flatten())\n",
    "# \n",
    "#     # Calculate inverse frequency for each class\n",
    "#     for class_label, count in class_counts.items():\n",
    "#         class_weights[class_label] = class_weights.get(class_label, 0) + count\n",
    "# \n",
    "# # Normalize class weights (optional but recommended)\n",
    "# total_sum = sum(class_weights.values())\n",
    "# for class_label, weight in class_weights.items():\n",
    "#     class_weights[class_label] = weight / total_sum\n",
    "# \n",
    "# inverted_weights = {class_label: 1.0 / weight for class_label, weight in class_weights.items()}\n",
    "# \n",
    "# total_sum = sum(inverted_weights.values())\n",
    "# for class_label, weight in inverted_weights.items():\n",
    "#     inverted_weights[class_label] = weight / total_sum"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cdfc8989db50ec18",
   "metadata": {},
   "source": [
    "# inverted_weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebb8e50ea6f5f2c1",
   "metadata": {},
   "source": [
    "def add_sample_weights(image, mask, class_weights):\n",
    "\n",
    "\n",
    "  # Assuming class_weights is a dictionary with class labels as keys and weights as values\n",
    "  #class_weights = {0: 2.0, 1: 2.0, 2: 1.0}  # Replace with your calculated weights\n",
    "\n",
    "  # Normalize class weights (optional but recommended)\n",
    "  # total_sum = sum(class_weights.values())\n",
    "  # for class_label, weight in class_weights.items():\n",
    "  #   class_weights[class_label] = weight / total_sum\n",
    "\n",
    "  # One-hot encode the mask (optional, depending on your loss function)\n",
    "  mask_one_hot = tf.one_hot(mask, depth=len(class_weights))\n",
    "\n",
    "  # Create the sample weight image by multiplying each class channel with its weight\n",
    "  sample_weights = mask_one_hot * tf.constant([class_weights[i] for i in range(len(class_weights))])\n",
    "\n",
    "  # Reduce the sample weight image to a single channel (if needed)\n",
    "  # sample_weights = tf.reduce_sum(sample_weights, axis=-1)  # Uncomment if needed\n",
    "\n",
    "  return image, mask, sample_weights\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ade0f263729c3374",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T23:14:53.920238Z",
     "start_time": "2024-05-03T23:14:53.920111Z"
    }
   },
   "source": [
    "# image, mask= read_image_mask(data_dir_train_image + 'images/case_9_CT-slice074_x.png', data_dir_train_mask + 'masks/case_9_CT-slice074_x.png')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1e96430cb8facb4",
   "metadata": {},
   "source": [
    "# image, mask, sample_weights = add_sample_weights(image, mask, iverted_class_weights)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fdda738956fcf40b",
   "metadata": {},
   "source": [
    "# train_generator = create_segmentation_generator_train(data_dir_train_image, data_dir_train_mask, BATCH_SIZE_TRAIN)\n",
    "# test_generator = create_segmentation_generator_test(data_dir_test_image, data_dir_test_mask, BATCH_SIZE_TEST)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "66c34ee0b0dcb062",
   "metadata": {},
   "source": [
    "#show_dataset(train_generator, 3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2f0ae8c5428a88fc",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "19dea77fa9425b31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:09:12.947664Z",
     "start_time": "2024-05-05T04:09:01.097284Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "class_weights = {}\n",
    "masks = os.listdir(data_dir_train_mask + 'masks/')\n",
    "# Assuming data_dir_train_mask is a list containing paths to your training masks\n",
    "total_count = len(masks)  # Total number of masks\n",
    "\n",
    "for mask_path in masks:\n",
    "    # Load the mask from the path (replace 'load_mask' with your loading function)\n",
    "    mask = cv2.imread(data_dir_train_mask + 'masks/'+mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Count the number of pixels for each class in the mask\n",
    "    class_counts = Counter(mask.flatten())\n",
    "\n",
    "    # Calculate inverse frequency for each class\n",
    "    for class_label, count in class_counts.items():\n",
    "        class_weights[class_label] = class_weights.get(class_label, 0) + count\n",
    "\n",
    "# Normalize class weights (optional but recommended)\n",
    "total_sum = sum(class_weights.values())\n",
    "for class_label, weight in class_weights.items():\n",
    "    class_weights[class_label] = weight / total_sum\n",
    "\n",
    "inverted_weights = {class_label: 1.0 / weight for class_label, weight in class_weights.items()}\n",
    "# \n",
    "# total_sum = sum(inverted_weights.values())\n",
    "# for class_label, weight in inverted_weights.items():\n",
    "#     inverted_weights[class_label] = weight / total_sum"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function AtomicFunction.__del__ at 0x7fe17a2048b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aziz0220/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 226, in __del__\n",
      "    self._bound_context.remove_function(self.name)\n",
      "  File \"/home/aziz0220/anaconda3/envs/py310/lib/python3.10/site-packages/tensorflow/python/eager/context.py\", line 1435, in remove_function\n",
      "    pywrap_tfe.TFE_ContextRemoveFunction(self._handle, name)\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7fe1b1d92080>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/aziz0220/anaconda3/envs/py310/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[126], line 12\u001B[0m\n\u001B[1;32m     10\u001B[0m mask \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(data_dir_train_mask \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmasks/\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m+\u001B[39mmask_path, cv2\u001B[38;5;241m.\u001B[39mIMREAD_GRAYSCALE)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# Count the number of pixels for each class in the mask\u001B[39;00m\n\u001B[0;32m---> 12\u001B[0m class_counts \u001B[38;5;241m=\u001B[39m \u001B[43mCounter\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# Calculate inverse frequency for each class\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m class_label, count \u001B[38;5;129;01min\u001B[39;00m class_counts\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/anaconda3/envs/py310/lib/python3.10/collections/__init__.py:577\u001B[0m, in \u001B[0;36mCounter.__init__\u001B[0;34m(self, iterable, **kwds)\u001B[0m\n\u001B[1;32m    566\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001B[39;00m\n\u001B[1;32m    567\u001B[0m \u001B[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001B[39;00m\n\u001B[1;32m    568\u001B[0m \u001B[38;5;124;03mof elements to their counts.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    574\u001B[0m \n\u001B[1;32m    575\u001B[0m \u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    576\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n\u001B[0;32m--> 577\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/py310/lib/python3.10/collections/__init__.py:670\u001B[0m, in \u001B[0;36mCounter.update\u001B[0;34m(self, iterable, **kwds)\u001B[0m\n\u001B[1;32m    668\u001B[0m             \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mupdate(iterable)\n\u001B[1;32m    669\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 670\u001B[0m         \u001B[43m_count_elements\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwds:\n\u001B[1;32m    672\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(kwds)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 126
  },
  {
   "cell_type": "code",
   "id": "b65e70014f623611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T23:14:53.926541Z",
     "start_time": "2024-05-03T23:14:53.926440Z"
    }
   },
   "source": [
    "# inverted_weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2b4199a42c203280",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:09:17.630736Z",
     "start_time": "2024-05-05T04:09:15.204544Z"
    }
   },
   "source": [
    "batch_size = 4\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "create_dir(\"files\")\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
    "print(f\"Train: {len(train_x)}/{len(train_y)} - Valid: {len(valid_x)}/{len(valid_y)} - Test: {len(test_x)}/{len(test_x)}\")\n",
    "print(\"\")\n",
    "\n",
    "train_ds = tf_dataset_train(train_x, train_y, batch=batch_size)\n",
    "valid_ds = tf_dataset_test(test_x, test_y, batch=batch_size)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 3706/3706 - Valid: 0/0 - Test: 927/927\n",
      "\n"
     ]
    }
   ],
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "id": "a44ab97832c58edc",
   "metadata": {},
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "820a012da13fd3ae",
   "metadata": {},
   "source": [
    "# for images, masks in train_ds.take(2):\n",
    "#   sample_image, sample_mask = images[0], masks[0,:,:,:3]\n",
    "#   display([sample_image, sample_mask])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9b2cfb0354ae88bc",
   "metadata": {},
   "source": [
    "#STEP 3 : BUILDING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f34acf917850c0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T17:19:55.321699Z",
     "start_time": "2024-05-04T17:19:55.043976Z"
    }
   },
   "source": "# model = load_or_build_unet(os.path.join(dataset_path + \"/filesfsdb\", 'set1.h5'), input_shape, num_classes)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found at /mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4/filesfsdb/set1.h5. Building a new model...\n",
      "Input shape: (512, 512, 3), Number of classes: 31\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "cell_type": "markdown",
   "id": "bef35c25daa592f6",
   "metadata": {},
   "source": [
    "# COMPILING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "id": "c4a92e9679c55dae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-03T23:14:53.933481Z",
     "start_time": "2024-05-03T23:14:53.933320Z"
    }
   },
   "source": [
    "# METRICS = [\n",
    "#     keras.losses.CategoricalCrossentropy(name='crossentropy'),\n",
    "#     keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "# ]\n",
    "# model.compile(\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     optimizer=tf.keras.optimizers.Adam(lr),\n",
    "#     metrics=METRICS  # Pass the list of metrics here\n",
    "# )\n",
    "# model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6b9d67ff8740656",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "id": "db148a9ba8660b22",
   "metadata": {},
   "source": [
    "# inverted_weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "last_case_number = 0\n",
    "dataset_path = '/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4'\n",
    "model = load_or_build_unet(os.path.join(dataset_path+f\"/files_{last_case_number}\", f\"model_{last_case_number}_epoch.h5\"), input_shape, num_classes)\n",
    "last_case_number += 1\n",
    "# METRICS = [\n",
    "#     #keras.losses.CategoricalCrossentropy(name='crossentropy'),\n",
    "#     keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "# ]\n",
    "model.compile(\n",
    "    # loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    #loss=\"categorical_crossentropy\",\n",
    "    loss=tf.keras.losses.CategoricalFocalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adam(lr),\n",
    "    # metrics=METRICS  # Pass the list of metrics here\n",
    ")"
   ],
   "id": "33595d1f3ee972f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.summary()",
   "id": "1b731f28a1aadab4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install pydot graphviz\n",
   "id": "f1e1d6e93b467dc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)",
   "id": "8f0939c740acec8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T04:18:08.589232Z",
     "start_time": "2024-05-05T04:18:01.762271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "last_case_number = 3\n",
    "while True:\n",
    "    dataset_path = '/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4'\n",
    "    model = load_or_build_unet(os.path.join(dataset_path+f\"/files_{last_case_number}\", f\"model_{last_case_number}_epoch.h5\"), input_shape, num_classes)\n",
    "    last_case_number += 1\n",
    "    # METRICS = [\n",
    "    #     #keras.losses.CategoricalCrossentropy(name='crossentropy'),\n",
    "    #     keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "    # ]\n",
    "    model.compile(\n",
    "        # loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "        #loss=\"categorical_crossentropy\",\n",
    "        loss=tf.keras.losses.CategoricalFocalCrossentropy(),\n",
    "        optimizer=tf.keras.optimizers.Adam(lr),\n",
    "        # metrics=METRICS  # Pass the list of metrics here\n",
    "    )\n",
    "    model_path = os.path.join(dataset_path ,f\"files_{last_case_number}\")\n",
    "    model_xx = os.path.join(model_path, f\"model_{last_case_number}_epoch.h5\")\n",
    "    csv_path = os.path.join(dataset_path+f\"/files_{last_case_number}\", \"data.csv\")\n",
    "    callbacks = [\n",
    "    ModelCheckpoint(model_xx, verbose=1, save_best_only=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, verbose=1),\n",
    "    CSVLogger(csv_path, append=True),\n",
    "    EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    ]\n",
    "    # model.fit(train_ds,\n",
    "    #     validation_data=valid_ds,\n",
    "    #     epochs=20,\n",
    "    #     callbacks=callbacks\n",
    "    #     # class_weight = inverted_weights\n",
    "    # )\n",
    "    # model.save(model_xx)\n",
    "    SCORE = []\n",
    "    hd_list = []\n",
    "    for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
    "        \"\"\" Extract the name \"\"\"\n",
    "        name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "        \"\"\" Reading the image \"\"\"\n",
    "        image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "        #image = cv2.resize(image, (image_w, image_h))\n",
    "        #image = 1-image\n",
    "        image_x = image\n",
    "        #image = image.astype(np.uint8)\n",
    "        image = image/255.0 ## (H, W, 3)\n",
    "        #image = np.stack((image,)*3, axis=-1)\n",
    "        image = np.expand_dims(image, axis=0) ## [1, H, W, 3]\n",
    "        image = image.astype(np.float32)\n",
    "        \"\"\" Reading the mask \"\"\"\n",
    "        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "        #mask = cv2.resize(mask, (image_w, image_h))\n",
    "        #mask = mask.astype(np.uint8)\n",
    "        mask = mask.astype(np.int32)\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        pred = model.predict(image, verbose=0)[0]\n",
    "        pred = np.argmax(pred, axis=-1) ## [0.1, 0.2, 0.1, 0.6] -> 3\n",
    "        pred = pred.astype(np.int32)\n",
    "         #cv2.imwrite(\"pred.png\", pred * (255/31))\n",
    "        # rgb_mask = grayscale_to_rgb(pred, rgb_codes)\n",
    "        # cv2.imwrite(\"pred.png\", rgb_mask)\n",
    "        \"\"\" Save the results \"\"\"\n",
    "        save_image_path = dataset_path+ f\"/results_{last_case_number}/{name}.png\"\n",
    "        save_results(image_x, mask, pred, save_image_path)\n",
    "        \"\"\" Flatten the array \"\"\"\n",
    "        mask = mask.flatten()\n",
    "        pred = pred.flatten()\n",
    "        labels = [i for i in range(num_classes)]\n",
    "        class_masks = []\n",
    "        class_preds = []\n",
    "        hd_list = np.empty(num_classes) \n",
    "        for c in range(num_classes):\n",
    "            class_mask = (mask == c).astype(np.uint8)\n",
    "            class_pred = (pred == c).astype(np.uint8)\n",
    "            # Calculate Hausdorff Distance for each class\n",
    "            hd = hausdorff_distance(class_pred, class_mask)\n",
    "            hd_list[c] = hd  # Assign hd to the corresponding index in hd_list\n",
    "        \"\"\" Calculating the metrics values \"\"\"\n",
    "        print(pred.shape)\n",
    "        metrics = sg.write_metrics(labels=labels,  # exclude background if needed\n",
    "                  gdth_img=mask,\n",
    "                  pred_img=pred,\n",
    "                  csv_file=dataset_path + f\"/files_{last_case_number}/hd95.csv\",  # save results to the csv_file \n",
    "                \n",
    "                  metrics=['hd', 'hd95', 'msd'])\n",
    "        f1_value = f1_score(mask, pred, labels=labels, average=None, zero_division=0)\n",
    "        jac_value = jaccard_score(mask, pred, labels=labels, average=None, zero_division=0)\n",
    "        SCORE.append([f1_value, jac_value, hd_list])\n",
    "    score = np.array(SCORE)\n",
    "    score = np.mean(score, axis=0)\n",
    "    f = open(dataset_path + f\"/files_{last_case_number}/score.csv\", \"w\")\n",
    "    f.write(\"Class,F1,Jaccard,HD95\\n\")\n",
    "    l = [\"Class\", \"F1\", \"Jaccard\", \"HD95\"]\n",
    "    print(f\"{l[0]:15s} {l[1]:10s} {l[2]:10s} {l[3]:10s}\")\n",
    "    print(\"-\"*35)\n",
    "    for i in range(num_classes):\n",
    "        class_name = classes[i]\n",
    "        f1 = score[0, i]\n",
    "        jac = score[1, i]\n",
    "        hd95 = score[2, i]\n",
    "        dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f} {hd95:1.5f}\"\n",
    "        print(dstr)\n",
    "        f.write(f\"{class_name:15s},{f1:1.5f},{jac:1.5f},{hd95:1.5f}\\n\")\n",
    "    print(\"-\"*35)\n",
    "    class_mean = np.mean(score, axis=-1)\n",
    "    class_name = \"Mean\"\n",
    "    f1 = class_mean[0]\n",
    "    jac = class_mean[1]\n",
    "    hd95 = class_mean[2]\n",
    "    dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f} - {hd95:1.5f}\"\n",
    "    print(dstr)\n",
    "    f.write(f\"{class_name:15s},{f1:1.5f},{jac:1.5f},{hd95:1.5f}\\n\")\n",
    "    f.close()"
   ],
   "id": "25ee4df5ebfda3ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing model from /mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4/files_3/model_3_epoch.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/927 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262144,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The dimension of gdth should be 2 or 3, but it is 1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[135], line 78\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\" Calculating the metrics values \"\"\"\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28mprint\u001B[39m(pred\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m---> 78\u001B[0m metrics \u001B[38;5;241m=\u001B[39m \u001B[43msg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# exclude background if needed\u001B[39;49;00m\n\u001B[1;32m     79\u001B[0m \u001B[43m          \u001B[49m\u001B[43mgdth_img\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     80\u001B[0m \u001B[43m          \u001B[49m\u001B[43mpred_img\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     81\u001B[0m \u001B[43m          \u001B[49m\u001B[43mcsv_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_path\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/files_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlast_case_number\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/hd95.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# save results to the csv_file \u001B[39;49;00m\n\u001B[1;32m     82\u001B[0m \u001B[43m        \u001B[49m\n\u001B[1;32m     83\u001B[0m \u001B[43m          \u001B[49m\u001B[43mmetrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhd\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhd95\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmsd\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     84\u001B[0m f1_value \u001B[38;5;241m=\u001B[39m f1_score(mask, pred, labels\u001B[38;5;241m=\u001B[39mlabels, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     85\u001B[0m jac_value \u001B[38;5;241m=\u001B[39m jaccard_score(mask, pred, labels\u001B[38;5;241m=\u001B[39mlabels, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/seg_metrics/seg_metrics.py:348\u001B[0m, in \u001B[0;36mwrite_metrics\u001B[0;34m(labels, gdth_path, pred_path, csv_file, gdth_img, pred_img, metrics, verbose, spacing, fully_connected, TPTNFPFN)\u001B[0m\n\u001B[1;32m    346\u001B[0m         gdth_spacing \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m1.\u001B[39m, \u001B[38;5;241m1.\u001B[39m, \u001B[38;5;241m1.\u001B[39m])  \u001B[38;5;66;03m# spacing should be double\u001B[39;00m\n\u001B[1;32m    347\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 348\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe dimension of gdth should be 2 or 3, but it is \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgdth\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    349\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    350\u001B[0m     gdth_spacing \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(spacing)\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat64)\n",
      "\u001B[0;31mException\u001B[0m: The dimension of gdth should be 2 or 3, but it is 1"
     ]
    }
   ],
   "execution_count": 135
  },
  {
   "cell_type": "code",
   "id": "2f251801fdd94b20",
   "metadata": {},
   "source": [
    "# test_classes = count_masks_per_class('/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4/data_case1_10epochs_training_imbalenced_classes/test/masks/masks', LABEL_dict)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9894869ddeb2325e",
   "metadata": {},
   "source": [
    "# class_counts = {name: 0 for name in LABEL_dict.keys()}\n",
    "# model_path = '/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4/data_case1_10epochs_training_imbalenced_classes/files_4/model_4_epoch.h5'\n",
    "# model = tf.keras.models.load_model(model_path)\n",
    "# path = '/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4/data_case1_10epochs_training_imbalenced_classes/test/images/images/'\n",
    "# for x in os.listdir(path):    \n",
    "#     \"\"\" Reading the image \"\"\"\n",
    "#     image = cv2.imread(path+x, cv2.IMREAD_COLOR)\n",
    "#     image = image/255.0 \n",
    "#     image = np.expand_dims(image, axis=0) ## [1, H, W, 3]\n",
    "#     image = image.astype(np.float32)\n",
    "#     \"\"\" Prediction \"\"\"\n",
    "#     pred = model.predict(image, verbose=0)[0]\n",
    "#     pred = np.argmax(pred, axis=-1) ## [0.1, 0.2, 0.1, 0.6] -> 3\n",
    "#     pred = pred.astype(np.int32)\n",
    "#     class_counts = class_to_dict(pred, class_counts, LABEL_dict)   \n",
    "# print(\"\\nclasses per original masks : \",test_classes)\n",
    "# print(\"\\nclasses per predicted masks : \",class_counts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e6fc677f16129c72",
   "metadata": {},
   "source": [
    "# STEP 4 : PREDICTION & EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "id": "c74fa5471d8d791d",
   "metadata": {},
   "source": [
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "# \n",
    "# \"\"\" Directory for storing files \"\"\"\n",
    "# create_dir(\"results\")\n",
    "# \n",
    "# \"\"\" Paths \"\"\"\n",
    "# dataset_path = '/mnt/c/Users/benam/Downloads/HaN-Seg/HaN-Seg/set_4'\n",
    "# model_path = os.path.join(dataset_path + \"/files_1\", 'model_1_epoch.h5')\n",
    "# \n",
    "# \"\"\" Loading the dataset \"\"\"\n",
    "# (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_dataset(dataset_path)\n",
    "# print(f\"Train: {len(train_x)}/{len(train_y)} - Valid: {len(valid_x)}/{len(valid_y)} - Test: {len(test_x)}/{len(test_x)}\")\n",
    "# print(\"\")\n",
    "# \n",
    "# \"\"\" Load the model \"\"\"\n",
    "# model = tf.keras.models.load_model(model_path)\n",
    "# \n",
    "# \"\"\" Prediction & Evaluation \"\"\"\n",
    "# SCORE = []\n",
    "# for x, y in tqdm(zip(test_x, test_y), total=len(test_x)):\n",
    "#     \"\"\" Extract the name \"\"\"\n",
    "#     name = x.split(\"/\")[-1].split(\".\")[0]\n",
    "# \n",
    "#     \"\"\" Reading the image \"\"\"\n",
    "#     image = cv2.imread(x, cv2.IMREAD_COLOR)\n",
    "#     #image = cv2.resize(image, (image_w, image_h))\n",
    "#     #image = 1-image\n",
    "#     image_x = image\n",
    "#     image = np.expand_dims(image, axis=0) ## [1, H, W, 3]\n",
    "#     image = image.astype(np.float32)\n",
    "#     \n",
    "#     #image = np.stack((image,)*3, axis=-1)\n",
    "#     #plt.imsave(dataset_path+f\"/resultimg04/{name}.png\", image, cmap='gray')\n",
    "#     \n",
    "#     image = np.expand_dims(image, axis=0) ## [1, H, W, 3]\n",
    "#     \n",
    "# \n",
    "#     \"\"\" Reading the mask \"\"\"\n",
    "#     mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)\n",
    "#     #mask = cv2.resize(mask, (image_w, image_h))\n",
    "#     #mask = np.stack((mask,)*3, axis=-1)\n",
    "#     mask = mask.astype(np.int32)\n",
    "# \n",
    "#     \"\"\" Prediction \"\"\"\n",
    "#     pred = model.predict(image, verbose=0)[0]\n",
    "#     pred = np.argmax(pred, axis=-1) ## [0.1, 0.2, 0.1, 0.6] -> 3\n",
    "#     pred = pred.astype(np.int32)\n",
    "# \n",
    "#     #cv2.imwrite(dataset_path+f\"/hhhhhhhhhhh/{name}.png\", image_x )\n",
    "#  \n",
    "#     # rgb_mask = grayscale_to_rgb(pred, rgb_codes)\n",
    "#     # cv2.imwrite(\"pred.png\", rgb_mask)\n",
    "# \n",
    "#     \"\"\" Save the results \"\"\"\n",
    "#     save_image_path = dataset_path+f\"/results_1/{name}.png\"\n",
    "#     save_results(image_x, mask, pred, save_image_path)\n",
    "# \n",
    "#     \"\"\" Flatten the array \"\"\"\n",
    "#     mask = mask.flatten()\n",
    "#     pred = pred.flatten()\n",
    "# \n",
    "#     labels = [i for i in range(num_classes)]\n",
    "# \n",
    "#     \"\"\" Calculating the metrics values \"\"\"\n",
    "#     f1_value = f1_score(mask, pred, labels=labels, average=None, zero_division=0)\n",
    "#     jac_value = jaccard_score(mask, pred, labels=labels, average=None, zero_division=0)\n",
    "#     SCORE.append([f1_value, jac_value])\n",
    "# \n",
    "# score = np.array(SCORE)\n",
    "# score = np.mean(score, axis=0)\n",
    "# \n",
    "# f = open(dataset_path+\"/files_1/score.csv\", \"w\")\n",
    "# f.write(\"Class,F1,Jaccard\\n\")\n",
    "# \n",
    "# l = [\"Class\", \"F1\", \"Jaccard\"]\n",
    "# print(f\"{l[0]:15s} {l[1]:10s} {l[2]:10s}\")\n",
    "# print(\"-\"*35)\n",
    "# \n",
    "# for i in range(num_classes):\n",
    "#     class_name = classes[i]\n",
    "#     f1 = score[0, i]\n",
    "#     jac = score[1, i]\n",
    "#     dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\n",
    "#     print(dstr)\n",
    "#     f.write(f\"{class_name:15s},{f1:1.5f},{jac:1.5f}\\n\")\n",
    "# \n",
    "# print(\"-\"*35)\n",
    "# class_mean = np.mean(score, axis=-1)\n",
    "# class_name = \"Mean\"\n",
    "# \n",
    "# f1 = class_mean[0]\n",
    "# jac = class_mean[1]\n",
    "# \n",
    "# dstr = f\"{class_name:15s}: {f1:1.5f} - {jac:1.5f}\"\n",
    "# print(dstr)\n",
    "# f.write(f\"{class_name:15s},{f1:1.5f},{jac:1.5f}\\n\")\n",
    "# \n",
    "# f.close()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
